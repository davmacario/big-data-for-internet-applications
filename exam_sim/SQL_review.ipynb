{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c7edf49-0c22-4a87-9180-fbb8ba459c5a",
   "metadata": {},
   "source": [
    "# Spark SQL review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea998490-296c-47d0-8c9a-b60f49c8780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "N_ELEM = 100\n",
    "# NOTE: randint includes the specified boudaries\n",
    "\n",
    "def gener_val():\n",
    "    return random.sample(range(0, 100), 5)\n",
    "\n",
    "ids = ['first','second', 'third', 'fourth', 'fifth']\n",
    "labels = ['aaa', 'bbb', 'c', 'ccc'] \n",
    "def gener_id():\n",
    "    return ids[random.randint(0, 4)]\n",
    "\n",
    "collection = []\n",
    "for i in range(N_ELEM):\n",
    "    new_el = gener_val()\n",
    "    new_el.insert(0, gener_id())\n",
    "    new_el.append(labels[random.randint(0, 3)])\n",
    "    \n",
    "    collection.append(new_el)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "209f7020-5168-4d75-802b-a020dbb05579",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ID', 'a', 'b', 'c', 'd', 'e', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85ec9b81-939a-4b11-b734-9918405a1fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+---+---+---+-----+\n",
      "|    ID|  a|  b|  c|  d|  e|label|\n",
      "+------+---+---+---+---+---+-----+\n",
      "| first|  0| 68| 42| 95| 65|  bbb|\n",
      "| first| 34| 83| 39| 88|  9|  ccc|\n",
      "| fifth| 59| 16| 15| 96| 64|    c|\n",
      "| fifth| 92| 43| 86| 34| 17|  bbb|\n",
      "| fifth| 26| 96| 57| 68| 35|    c|\n",
      "| first| 24| 81| 97| 88| 51|  ccc|\n",
      "| fifth| 75| 28| 91| 29| 61|  ccc|\n",
      "| third| 32|  9| 15| 48| 78|    c|\n",
      "|fourth| 93| 29| 17| 28| 80|  aaa|\n",
      "| fifth| 38| 87| 80| 63| 60|    c|\n",
      "+------+---+---+---+---+---+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_DF = spark.createDataFrame(collection, cols)\n",
    "in_DF.createOrReplaceTempView('starting')\n",
    "in_DF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "effd04a4-5858-4acc-ac3a-18556b4c3285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'a', 'b', 'c', 'd', 'e', 'label']\n",
      "Column<b'ID'>\n"
     ]
    }
   ],
   "source": [
    "# Attributes:\n",
    "print(in_DF.columns)\n",
    "print(in_DF.ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e400d968-b74c-4e86-b261-0dbde5e3aedf",
   "metadata": {},
   "source": [
    "## Aggregate functions without groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94f4de2f-ea13-4b71-8804-4514918708e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|min(a)|\n",
      "+------+\n",
      "|     1|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Can use aggregate functions WITHOUT GROUPBY\n",
    "in_DF.agg({'a':'sum','a':'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5998977-2375-451f-a40f-3c0a95c775cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|sum(a)|\n",
      "+------+\n",
      "|  5482|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It is the same as using selectExpr() passing the column to the \n",
    "# aggregate function:\n",
    "in_DF.selectExpr('sum(a)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4223e288-f42a-4ba3-b8f9-374ef47604c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a786eaf-d938-4958-8a44-4b4c3b82ecb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|    ID|flt_a|\n",
      "+------+-----+\n",
      "| first| 98.0|\n",
      "|fourth|  5.0|\n",
      "| third| 23.0|\n",
      "| third| 45.0|\n",
      "| fifth| 81.0|\n",
      "|second| 82.0|\n",
      "|second| 79.0|\n",
      "|second| 50.0|\n",
      "|fourth| 88.0|\n",
      "| first| 47.0|\n",
      "+------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_DF.selectExpr(\"ID\", \"CAST(a AS float) AS flt_a\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b0f2ab-cb35-414d-a146-42d4e55fcf39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d6f1bd-dab0-41ee-9ccd-ce6390abb584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0da17-033e-4efd-a51e-20755bd61229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da4baf8d-ef93-46e4-a02c-4025d8ff8b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|avg(a)|\n",
      "+------+\n",
      "| 53.22|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_DF.selectExpr(\"avg(a)\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09a8d4-b5bb-4649-a905-f52421077268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb962748-6317-4d60-979a-848583fc189c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+---+---+---+---+-----+---------+\n",
      "|    ID|  a|  b|  c|  d|  e|label|ind_label|\n",
      "+------+---+---+---+---+---+-----+---------+\n",
      "| first| 98|  6| 63|  2| 15|    c|      3.0|\n",
      "|fourth|  5| 14| 64| 36| 93|    c|      3.0|\n",
      "| third| 23| 67| 90| 24| 64|  aaa|      1.0|\n",
      "| third| 45| 35| 92|  0| 90|  ccc|      2.0|\n",
      "| fifth| 81| 90| 57| 69| 78|  bbb|      0.0|\n",
      "|second| 82| 87| 57|  8| 12|  ccc|      2.0|\n",
      "|second| 79| 15| 82| 12| 56|  bbb|      0.0|\n",
      "|second| 50| 34| 58| 65| 59|  bbb|      0.0|\n",
      "|fourth| 88| 21| 90| 60| 62|  aaa|      1.0|\n",
      "| first| 47| 73| 88| 50| 81|  ccc|      2.0|\n",
      "+------+---+---+---+---+---+-----+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+---+---+---+---+---+-----+---------+-------+\n",
      "|    ID|  a|  b|  c|  d|  e|label|ind_label|label_2|\n",
      "+------+---+---+---+---+---+-----+---------+-------+\n",
      "| first| 98|  6| 63|  2| 15|    c|      3.0|      c|\n",
      "|fourth|  5| 14| 64| 36| 93|    c|      3.0|      c|\n",
      "| third| 23| 67| 90| 24| 64|  aaa|      1.0|    aaa|\n",
      "| third| 45| 35| 92|  0| 90|  ccc|      2.0|    ccc|\n",
      "| fifth| 81| 90| 57| 69| 78|  bbb|      0.0|    bbb|\n",
      "|second| 82| 87| 57|  8| 12|  ccc|      2.0|    ccc|\n",
      "|second| 79| 15| 82| 12| 56|  bbb|      0.0|    bbb|\n",
      "|second| 50| 34| 58| 65| 59|  bbb|      0.0|    bbb|\n",
      "|fourth| 88| 21| 90| 60| 62|  aaa|      1.0|    aaa|\n",
      "| first| 47| 73| 88| 50| 81|  ccc|      2.0|    ccc|\n",
      "+------+---+---+---+---+---+-----+---------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# String indexer and index to string\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "\n",
    "str2ind = StringIndexer(inputCol='label', outputCol='ind_label').fit(in_DF)\n",
    "no_cat_DF = str2ind.transform(in_DF)\n",
    "no_cat_DF.show(10)\n",
    "\n",
    "ind2str = IndexToString(inputCol='ind_label', outputCol='label_2')\n",
    "ret_DF = ind2str.transform(no_cat_DF)    # no need to fit!!!\n",
    "ret_DF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71a878-79bc-48f7-b3e6-be9fbf2ace29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807cf4f7-0970-4597-a8a6-509473be407d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc75e097-ced9-41a5-8f2c-c332b22b897a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6db62-97a7-4272-945b-5a9eda112637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a38fa3-2f0d-4d47-bf6a-d7273f725309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0069abbd-3b8e-464c-b286-7b5e42fa211b",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "420e4a7e-c6a7-4029-9b21-fc2223bb721b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                     (0 + 134) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|    ID|count(1)|\n",
      "+------+--------+\n",
      "| first|      24|\n",
      "|fourth|      23|\n",
      "| fifth|      20|\n",
      "| third|      18|\n",
      "|second|      15|\n",
      "+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Spark SQL\n",
    "sec_DF = in_DF.groupBy('ID').agg({'*': 'count'}).sort('count(1)', ascending=False)\n",
    "sec_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c39457a9-fad4-4812-b6c2-910cf7c8b4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:======>                                                 (8 + 64) / 72]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|    ID|counts|\n",
      "+------+------+\n",
      "| first|    24|\n",
      "|fourth|    23|\n",
      "| fifth|    20|\n",
      "| third|    18|\n",
      "|second|    15|\n",
      "+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# SQL\n",
    "sec_DF_2 = spark.sql(\"\"\"\n",
    "                        SELECT ID, count(*) AS counts\n",
    "                        FROM starting\n",
    "                        GROUP BY ID\n",
    "                        ORDER BY -1*counts\n",
    "                    \"\"\")\n",
    "\n",
    "sec_DF_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f700e1a-e182-44ea-a638-45f4b765c1d9",
   "metadata": {},
   "source": [
    "## 2 - UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b01b970b-9331-4b67-a974-abd8cc2fcc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(c1, c2)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UDF\n",
    "spark.udf.register('myTest', lambda c1, c2: (c1+c2)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f73452d0-e919-4edc-b7ee-88614563993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|    ID|avg_ab|\n",
      "+------+------+\n",
      "| first|  52.0|\n",
      "|fourth|   9.5|\n",
      "| third|  45.0|\n",
      "| third|  40.0|\n",
      "| fifth|  85.5|\n",
      "|second|  84.5|\n",
      "|second|  47.0|\n",
      "|second|  42.0|\n",
      "|fourth|  54.5|\n",
      "| first|  60.0|\n",
      "+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_DF.selectExpr('ID', 'myTest(a, b) AS avg_ab').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ce99c-9fbb-48db-971e-85a1d244bef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deb94d5e-951a-4765-999c-fef3f8b73828",
   "metadata": {},
   "source": [
    "## 3 - Extracting a single column\n",
    "\n",
    "Goal: get a single column of the DF as local Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dfa802e-2a23-49ac-a936-46be3c62a3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98, 5, 23, 45, 81, 82, 79, 50, 88, 47, 62, 52, 93, 90, 44, 66, 63, 85, 40, 87, 2, 58, 91, 99, 9, 94, 35, 51, 69, 76, 83, 37, 51, 52, 71, 24, 44, 62, 40, 51, 26, 43, 99, 76, 30, 79, 92, 35, 67, 70, 71, 42, 12, 75, 53, 52, 52, 96, 75, 28, 18, 39, 67, 44, 34, 26, 41, 17, 25, 12, 49, 9, 9, 89, 97, 15, 1, 36, 32, 99, 13, 28, 93, 62, 78, 18, 3, 64, 69, 83, 78, 28, 87, 55, 41, 55, 29, 26, 16, 85]\n"
     ]
    }
   ],
   "source": [
    "listOfRows = in_DF.select('a').collect()\n",
    "\n",
    "print([el['a'] for el in listOfRows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ec7ff-c5e8-4cf4-91a6-daf506782b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5eaf712d-5244-4de2-b99c-f7fb24489f90",
   "metadata": {},
   "source": [
    "## 4 - Counting the elements which satisfy a certain condition\n",
    "\n",
    "Possibilities: \n",
    "\n",
    "* `count(col_name > number)` $\\rightarrow$ It simply counts the number of rows...\n",
    "* `sum(CAST(col_name > number AS int))` $\\rightarrow$ **CORRECT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c59f3ab-1039-4f60-99a0-0ff993cf8f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------------------+\n",
      "|    ID|count(a)|sum(CAST((label = aaa) AS INT))|\n",
      "+------+--------+-------------------------------+\n",
      "|fourth|       8|                              2|\n",
      "| fifth|       8|                              2|\n",
      "|second|      14|                              7|\n",
      "| third|      13|                              1|\n",
      "| first|      11|                              4|\n",
      "+------+--------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get elements having 'a' > 'b' and numbers of 'aaa' > 3\n",
    "\n",
    "\n",
    "# First method: IT DOES NOT WORK\n",
    "spark.sql(\"\"\"\n",
    "        SELECT ID, count(a), sum(CAST(label == 'aaa' AS int))\n",
    "        FROM starting\n",
    "        WHERE a > b\n",
    "        GROUP BY ID\n",
    "        HAVING count(label == 'aaa') > 9\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da606907-a632-48bb-abe2-a5bfc2848f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------------------------------+\n",
      "|    ID|sum(a)|sum(b)|sum(CAST((label = aaa) AS INT))|\n",
      "+------+------+------+-------------------------------+\n",
      "|second|  1036|   586|                              7|\n",
      "| first|   834|   370|                              4|\n",
      "+------+------+------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second\n",
    "spark.sql(\"\"\"\n",
    "        SELECT ID, sum(a), sum(b), sum(CAST(label == 'aaa' AS int))\n",
    "        FROM starting\n",
    "        WHERE a > b\n",
    "        GROUP BY ID\n",
    "        HAVING sum(CAST(label == 'aaa' AS int)) > 2\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4347239-11e2-47f9-9b49-2b8a865d50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_DF.filter(\"a > b\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436b11d1-7dfd-4076-b4fb-7b3f57a7f96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae1167e5-84d4-4ac5-8792-993f0252c9aa",
   "metadata": {},
   "source": [
    "## Using distinct in agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a3a3e4e-16aa-4caf-9621-0a3621b07ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+\n",
      "|    ID|count(DISTINCT a)|\n",
      "+------+-----------------+\n",
      "|fourth|                8|\n",
      "| fifth|                8|\n",
      "|second|               12|\n",
      "| third|               11|\n",
      "| first|               10|\n",
      "+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT ID, count(distinct(a)) \n",
    "        FROM starting\n",
    "        WHERE a > b\n",
    "        GROUP BY ID\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e181c-cc87-40d3-bf7f-ab7cad0a53f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d8bed-fc0e-4fe1-985a-cf08b72a5bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34ddd32a-25e6-41e8-9b6a-ced96266611d",
   "metadata": {},
   "source": [
    "## On SQL queries - simulations doubts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3829069-2dfd-458f-a839-1a190308566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+\n",
      "|    ID|new_a|sum(b)|\n",
      "+------+-----+------+\n",
      "| third|    0|   353|\n",
      "| first|    1|   186|\n",
      "| fifth|    1|   247|\n",
      "|fourth|    0|   211|\n",
      "|second|    0|   212|\n",
      "|fourth|    1|   447|\n",
      "| first|    0|   252|\n",
      "|second|    1|   519|\n",
      "| fifth|    0|   163|\n",
      "| third|    1|   401|\n",
      "+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 'starting' table\n",
    "\n",
    "spark.udf.register('myFunc', lambda n: int(n%2))\n",
    "\n",
    "# TO INCLUDE NEW COLUMNS IN GROUP BY MUST USE ALIAS!\n",
    "spark.sql(\"\"\"\n",
    "        SELECT ID, myFunc(a) AS new_a, sum(b)\n",
    "        FROM starting\n",
    "        WHERE label == 'aaa' OR label == 'bbb'\n",
    "        GROUP BY ID, new_a\n",
    "\"\"\").show(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b842b03a-1474-4ab6-a012-effb3ea98a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|    ID|new_a|\n",
      "+------+-----+\n",
      "| third|    0|\n",
      "| fifth|    1|\n",
      "|fourth|    0|\n",
      "|second|    0|\n",
      "|fourth|    1|\n",
      "| first|    0|\n",
      "|second|    1|\n",
      "| third|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HAVING columns not in select\n",
    "spark.sql(\"\"\"\n",
    "        SELECT ID, myFunc(a) AS new_a\n",
    "        FROM starting\n",
    "        WHERE label == 'aaa' OR label == 'bbb'\n",
    "        GROUP BY ID, new_a\n",
    "        HAVING sum(b) > 200\n",
    "\"\"\").show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44760f96-7526-46f1-ac6a-02cf6da382d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|sum(b)|\n",
      "+------+\n",
      "|   353|\n",
      "|   186|\n",
      "|   247|\n",
      "|   211|\n",
      "|   212|\n",
      "|   447|\n",
      "|   252|\n",
      "|   519|\n",
      "|   163|\n",
      "|   401|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NO NEED TO INCLUDE GROUP BY COLS IN SELECT\n",
    "spark.sql(\"\"\"\n",
    "        SELECT sum(b)\n",
    "        FROM starting\n",
    "        WHERE label == 'aaa' OR label == 'bbb'\n",
    "        GROUP BY ID, myFunc(a)\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60736b22-2e45-40ae-85bc-3b4bce3782a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|func_a|sum_b|\n",
      "+------+-----+\n",
      "|     1|  514|\n",
      "|     0|  396|\n",
      "|     1|  335|\n",
      "|     0|  321|\n",
      "|     0|  277|\n",
      "|     1|  260|\n",
      "|     0|  181|\n",
      "|     1|   66|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Column name in GROUP BY must be the alias - HAVING columns don't have to appear in SELECT\n",
    "# count(distinct(col)) is allowed\n",
    "spark.sql(\"\"\"\n",
    "        SELECT myFunc(a) AS func_a, sum(b) AS sum_b\n",
    "        FROM starting\n",
    "        WHERE label == 'aaa' OR label == 'bbb'\n",
    "        GROUP BY ID, func_a\n",
    "        HAVING count(distinct(label)) == 2\n",
    "        ORDER BY -1*sum_b\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c46fdd12-3f0b-4c22-acbc-ac3c8c041448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|    ID|\n",
      "+------+\n",
      "|fourth|\n",
      "| fifth|\n",
      "| third|\n",
      "| first|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_DF1 = spark.sql(\"\"\"\n",
    "        SELECT ID\n",
    "        FROM starting\n",
    "        WHERE label == 'aaa' OR label == 'bbb'\n",
    "        GROUP BY ID\n",
    "        HAVING sum(b) > 200\n",
    "\"\"\")\n",
    "\n",
    "my_DF1.createOrReplaceTempView('t1')\n",
    "\n",
    "my_DF1.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f9dd6c2-c172-4c39-9629-8ae8c3c26eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/03 07:54:18 WARN analysis.SimpleFunctionRegistry: The function checkfunc replaced a previously registered function.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|    ID|max(a)|\n",
      "+------+------+\n",
      "|fourth|    93|\n",
      "| fifth|    86|\n",
      "| first|    81|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register('checkFunc', lambda lab: lab.startswith('f'))\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        SELECT starting.ID, max(starting.a)\n",
    "        FROM starting, t1\n",
    "        WHERE starting.ID == t1.ID AND starting.label == 'aaa' AND checkFunc(t1.ID) == True\n",
    "        GROUP BY starting.ID\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609dd812-32a6-4595-b1f1-ed0a164856ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eea2023-40bc-4e4f-90d0-6f25aab11242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0137ae-0f75-48c9-ac16-bd97fef671a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae40eb12-a979-4cb8-991e-496890fd5d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d25d0-c170-4f9d-a879-d54231eb4a57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
