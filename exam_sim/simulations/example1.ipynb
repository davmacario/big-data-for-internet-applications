{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2698538f-eadf-4196-806e-51af7aecac58",
   "metadata": {},
   "source": [
    "# Exam example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc39ee2e-63ea-432d-8f45-1e18d6a0a0b3",
   "metadata": {},
   "source": [
    "## MCQ\n",
    "\n",
    "1. [b] - since the blocks cannot contain pieces of different files, the 524 MB file needs to be divided into 2 blocks\n",
    "2. [a] - 2 actions are performed on logsRDD (1 is actually on one of its descendants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46cd73e-b8af-40a6-88ff-492a83c17301",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9258d91-efb3-413d-9033-4db353c0dc17",
   "metadata": {},
   "source": [
    "### Ex01\n",
    "\n",
    "Structure of `prices.txt`:\n",
    "\n",
    "- Sampling rate: 5 minutes\n",
    "- Line format: `stockID,date,hour:minute,price`\n",
    "\n",
    "Analyze behavior for year 2016 - how many times each stock appeared with a price > 10€?\n",
    "\n",
    "Count in how many distinct dates each stock was > 10€; only keep the ones which appear >5 times.\n",
    "\n",
    "Output file structure:\n",
    "\n",
    "    stockID,n_dates_more_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc80d181-6edd-4bb9-8cc9-b1c24b2cc901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path\n",
    "##### actually: \n",
    "# in_file = sys.argv[1]\n",
    "in_file = '/data/students/bigdata_internet/exam_examples_data/example1_data/Prices.txt'\n",
    "\n",
    "# out_file = sys.argv[2]\n",
    "out_file = '/user/s315054/exam_sim/example1/p01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e11d55b-2a73-45ec-94dc-2fd7334416b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/02 20:57:11 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s315054/exam_sim/example1/p01' to trash at: hdfs://BigDataHA/user/s315054/.Trash/Current/user/s315054/exam_sim/example1/p01\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/s315054/exam_sim/example1/p01/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e262bd03-35ad-4b47-bc16-59762890d6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+-----+\n",
      "|stockID|      date| time|price|\n",
      "+-------+----------+-----+-----+\n",
      "|   FCAU|2016/06/20|16:10|10.43|\n",
      "|   FCAU|2016/06/20|16:15|10.45|\n",
      "|   FCAU|2016/06/20|16:20|10.53|\n",
      "|   FCAU|2016/06/20|16:25| 11.0|\n",
      "|   FCAU|2016/06/20|16:30|10.99|\n",
      "+-------+----------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "in_DF = spark.read.load(in_file, format='csv', sep=',', header=False, inferSchema=True)\\\n",
    "        .withColumnRenamed('_c0', 'stockID')\\\n",
    "        .withColumnRenamed('_c1', 'date')\\\n",
    "        .withColumnRenamed('_c2', 'time')\\\n",
    "        .withColumnRenamed('_c3', 'price')\n",
    "in_DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "542da3c8-710a-4b41-9340-b890e7ac4ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove elements which are not in 2016:\n",
    "spark.udf.register('getYear', lambda date: date.split('/')[0])\n",
    "\n",
    "df2016 = in_DF.select('stockID', 'date', 'price')\\\n",
    "                .filter('getYear(date) == 2016 AND price > 10')\\\n",
    "                .select('stockID', 'date')\\\n",
    "                .distinct()\n",
    "# Only id, date and price of 2016 records with price > 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b683f034-1b8f-4300-8916-e7b472bd1f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_more_10 = df2016.groupBy('stockID').agg({'*':'count'}).withColumnRenamed('count(1)', 'n_date_more_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40d8a95c-25bf-487b-ab7e-ebe70d03d88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_more_10.write.csv(out_file, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22978a12-1385-4b30-8229-5c057b749b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|stockID|n_date_more_10|\n",
      "+-------+--------------+\n",
      "|   GOOG|             8|\n",
      "|   FCAU|             7|\n",
      "|   AMZN|             1|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_more_10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aed359-2e36-4e23-be52-7b61badf0a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdd649a7-7513-450e-884b-c2cbd4a54538",
   "metadata": {},
   "source": [
    "### Ex02\n",
    "\n",
    "Select the highest price for each pair (stock, date) in 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2249f87c-a774-47e7-bc3a-ebbbe70e12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_1 = '/user/s315054/exam_sim/example1/p02_a'\n",
    "out_2 = '/user/s315054/exam_sim/example1/p02_b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e3c9f19-34b9-4840-a2a9-651e676eff73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/02 21:33:22 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s315054/exam_sim/example1/p02_a' to trash at: hdfs://BigDataHA/user/s315054/.Trash/Current/user/s315054/exam_sim/example1/p02_a\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/s315054/exam_sim/example1/p02_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e80281c1-4ed0-4789-806a-3000db66ee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/02 21:33:25 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s315054/exam_sim/example1/p02_b' to trash at: hdfs://BigDataHA/user/s315054/.Trash/Current/user/s315054/exam_sim/example1/p02_b\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/s315054/exam_sim/example1/p02_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea75fa-8c56-439d-967e-c5a297bbc814",
   "metadata": {},
   "source": [
    "#### Part a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d9e7007-219e-4b42-b2ba-b3d50836a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2016_only = in_DF.selectExpr('stockID', 'date', 'CAST(price AS float)')\\\n",
    "                    .filter('getYear(date) == 2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcf12d15-61d8-4928-8b7b-505b2334909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute for each date the highest price, then sort\n",
    "# Note: no need to transform date format - it works correctly\n",
    "max_price_DF = df_2016_only.groupBy('stockID', 'date').agg({'price':'max'}).sort('stockID', 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edefd982-e4e2-4691-9a7e-df249c39636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_price_DF.write.csv(out_1, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c416de21-9584-40d7-b22c-c2a72988ddcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+\n",
      "|stockID|      date|max(price)|\n",
      "+-------+----------+----------+\n",
      "|   AMZN|2016/01/04|      14.0|\n",
      "|   FCAU|2016/06/20|      11.0|\n",
      "|   FCAU|2016/06/21|      12.0|\n",
      "|   FCAU|2016/07/20|      11.0|\n",
      "|   FCAU|2016/07/21|      12.0|\n",
      "|   FCAU|2016/12/01|      9.95|\n",
      "|   FCAU|2016/12/02|       9.9|\n",
      "|   FCAU|2016/12/03|      10.1|\n",
      "|   FCAU|2016/12/04|      10.5|\n",
      "|   FCAU|2016/12/05|     10.53|\n",
      "|   GOOG|2016/01/01|      43.0|\n",
      "|   GOOG|2016/01/02|      45.0|\n",
      "|   GOOG|2016/01/03|      46.0|\n",
      "|   GOOG|2016/01/04|      40.0|\n",
      "|   GOOG|2016/06/20|      51.0|\n",
      "|   GOOG|2016/06/21|     51.99|\n",
      "|   GOOG|2016/07/20|     54.99|\n",
      "|   GOOG|2016/07/21|     51.99|\n",
      "+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_price_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b8b95-f8d7-4132-b928-a41a4c4a5b6a",
   "metadata": {},
   "source": [
    "#### Part b\n",
    "\n",
    "Positive weekly trend: difference between highest stock price of last day and the one of the first one is > 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ea1b9d3d-f63f-4d65-b55a-e7378736a73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/24 11:54:53 WARN analysis.SimpleFunctionRegistry: The function getweek replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "# Start from the 2016 dataframe with max prices\n",
    "from datetime import datetime\n",
    "\n",
    "# Goal: stock IDs of ones having at least 'NW' p.w.t.\n",
    "\n",
    "#NW = sys.argv[3]\n",
    "NW = 1\n",
    "\n",
    "# Add week numbers\n",
    "spark.udf.register('getWeek', lambda date: datetime.strptime(date, '%Y/%m/%d').strftime('%V'))\n",
    "\n",
    "week_DF = max_price_DF.withColumnRenamed('max(price)', 'max_price')\\\n",
    "            .selectExpr('stockID', 'date', 'getWeek(date) AS week', 'max_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1dc76544-17c8-47ab-8c65-d147e8058715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+----------+\n",
      "|stockID|week|     first|      last|\n",
      "+-------+----+----------+----------+\n",
      "|   FCAU|  25|2016/06/20|2016/06/21|\n",
      "|   GOOG|  01|2016/01/04|2016/01/04|\n",
      "|   FCAU|  49|2016/12/05|2016/12/05|\n",
      "|   FCAU|  48|2016/12/01|2016/12/04|\n",
      "|   GOOG|  53|2016/01/01|2016/01/03|\n",
      "+-------+----+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This part is NOT EFFICIENT\n",
    "####################################################\n",
    "# Get 1st day of the week by taking the min 'date' in each week\n",
    "first_days_DF = week_DF.groupBy('stockID', 'week').agg({'date':'min'}).withColumnRenamed('min(date)', 'first')\n",
    "\n",
    "# Get last day of the week by taking the max 'date' in each week\n",
    "last_days_DF = week_DF.groupBy('stockID', 'week').agg({'date':'max'}).withColumnRenamed('max(date)', 'last')\n",
    "\n",
    "# Now to get the prices join on the date and stockID\n",
    "max_and_min_DF = first_days_DF.join(last_days_DF, ['stockID', 'week'], 'inner')\n",
    "\n",
    "max_and_min_DF.show(5)\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f17f9199-89d3-434a-8570-3dd93336b02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+----------+\n",
      "|stockID|week|      last|     first|\n",
      "+-------+----+----------+----------+\n",
      "|   FCAU|  25|2016/06/21|2016/06/20|\n",
      "|   GOOG|  01|2016/01/04|2016/01/04|\n",
      "|   FCAU|  49|2016/12/05|2016/12/05|\n",
      "|   FCAU|  48|2016/12/04|2016/12/01|\n",
      "|   GOOG|  53|2016/01/03|2016/01/01|\n",
      "+-------+----+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "week_DF.createOrReplaceTempView('table_weeks')\n",
    "\n",
    "max_and_min_DF = spark.sql(\"\"\"\n",
    "                        SELECT stockID, week, max(date) AS last, min(date) AS first\n",
    "                        FROM table_weeks\n",
    "                        GROUP BY stockID, week\"\"\")\n",
    "\n",
    "max_and_min_DF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "923d216e-dd19-4daa-82eb-5849f5725cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----+-----------+----------+\n",
      "|stockID|      last|     first|week|price_first|price_last|\n",
      "+-------+----------+----------+----+-----------+----------+\n",
      "|   AMZN|2016/01/04|2016/01/04|  01|       14.0|      14.0|\n",
      "|   FCAU|2016/06/21|2016/06/20|  25|       11.0|      12.0|\n",
      "|   FCAU|2016/07/21|2016/07/20|  29|       11.0|      12.0|\n",
      "|   FCAU|2016/12/04|2016/12/01|  48|       9.95|      10.5|\n",
      "|   FCAU|2016/12/05|2016/12/05|  49|      10.53|     10.53|\n",
      "|   GOOG|2016/01/03|2016/01/01|  53|       43.0|      46.0|\n",
      "|   GOOG|2016/01/04|2016/01/04|  01|       40.0|      40.0|\n",
      "|   GOOG|2016/06/21|2016/06/20|  25|       51.0|     51.99|\n",
      "|   GOOG|2016/07/21|2016/07/20|  29|      54.99|     51.99|\n",
      "+-------+----------+----------+----+-----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#### MY SOLUTION\n",
    "\n",
    "max_and_min_prices = max_and_min_DF.join(week_DF.selectExpr('stockID', 'date AS first', 'max_price AS price_first'), ['stockID', 'first'])\\\n",
    ".join(week_DF.selectExpr('stockID', 'date AS last', 'max_price AS price_last'), ['stockID', 'last'])\n",
    "\n",
    "max_and_min_prices.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "fcb9ad43-34e6-419e-940b-bdc73a5cde0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-----------+----------+----------+\n",
      "|stockID|week|price_last|price_first|      last|     first|\n",
      "+-------+----+----------+-----------+----------+----------+\n",
      "|   AMZN|  01|      14.0|       14.0|2016/01/04|2016/01/04|\n",
      "|   FCAU|  25|      12.0|       11.0|2016/06/21|2016/06/20|\n",
      "|   FCAU|  29|      12.0|       11.0|2016/07/21|2016/07/20|\n",
      "|   FCAU|  48|      10.5|       9.95|2016/12/04|2016/12/01|\n",
      "|   FCAU|  49|     10.53|      10.53|2016/12/05|2016/12/05|\n",
      "|   GOOG|  53|      46.0|       43.0|2016/01/03|2016/01/01|\n",
      "|   GOOG|  01|      40.0|       40.0|2016/01/04|2016/01/04|\n",
      "|   GOOG|  25|     51.99|       51.0|2016/06/21|2016/06/20|\n",
      "|   GOOG|  29|     51.99|      54.99|2016/07/21|2016/07/20|\n",
      "+-------+----+----------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### ACTUAL SOLUTION\n",
    "max_and_min_DF.createOrReplaceTempView('max_min')\n",
    "\n",
    "max_and_min_prices_2 = spark.sql(\"\"\" \n",
    "                                SELECT max_min.stockID, max_min.week, table_weeks.max_price AS price_first, max_min.last, max_min.first\n",
    "                                FROM max_min, table_weeks\n",
    "                                WHERE max_min.stockID == table_weeks.stockID AND max_min.first == table_weeks.date\n",
    "                                \"\"\")\n",
    "\n",
    "max_and_min_prices_2.createOrReplaceTempView('t1')\n",
    "\n",
    "max_and_min_prices = spark.sql(\"\"\"\n",
    "                            SELECT t1.stockID, t1.week, table_weeks.max_price AS price_last, t1.price_first, t1.last, t1.first\n",
    "                            FROM t1, table_weeks\n",
    "                            WHERE t1.stockID == table_weeks.stockID AND t1.last == table_weeks.date\n",
    "                            \"\"\")\n",
    "\n",
    "\n",
    "max_and_min_prices.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b057b94f-6d63-4fbb-9a49-2ef7f05bce25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-----------+----------+----------+\n",
      "|stockID|week|price_last|price_first|      last|     first|\n",
      "+-------+----+----------+-----------+----------+----------+\n",
      "|   FCAU|  25|      12.0|       11.0|2016/06/21|2016/06/20|\n",
      "|   FCAU|  29|      12.0|       11.0|2016/07/21|2016/07/20|\n",
      "|   FCAU|  48|      10.5|       9.95|2016/12/04|2016/12/01|\n",
      "|   GOOG|  53|      46.0|       43.0|2016/01/03|2016/01/01|\n",
      "|   GOOG|  25|     51.99|       51.0|2016/06/21|2016/06/20|\n",
      "+-------+----+----------+-----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Isolate records for pos. week. trends:\n",
    "pwt_DF = max_and_min_prices.filter(\"price_first < price_last\")\n",
    "\n",
    "pwt_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "66534c07-510c-4c89-bfb3-3ebfd4947d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the n. of pwt for each stock, filter directly\n",
    "selected_DF = pwt_DF.groupBy('stockID').agg({'*':'count'}).filter(f'count(1) >= {NW}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a6de070e-610b-4af0-a339-245a6c5a880e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|stockID|count(1)|\n",
      "+-------+--------+\n",
      "|   GOOG|       2|\n",
      "|   FCAU|       3|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5165c458-3c35-498e-bd30-e97a2db29dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "selected_DF.select('stockID').write.csv(out_2, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "056f89ad-6f60-4615-8c11-80ec4cfdf437",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12102a74-66a0-4d03-a693-b7a8e0552b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c48018-fdd5-4609-8645-305bf7fdb889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8521c0-698a-4919-b22f-8f0d4a0a9edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
