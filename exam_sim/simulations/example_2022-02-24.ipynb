{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea06fc8-741d-4ae9-8d19-5f29ea3e743d",
   "metadata": {},
   "source": [
    "# Exam - Feb, 24th 2022\n",
    "\n",
    "## Part 1 - MCQ\n",
    "\n",
    "1. d - K-Means is a clusterig algorithm\n",
    "2. c - 5 blocks\n",
    "\n",
    "## Part 2 - programming\n",
    "\n",
    "### Ex 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c874d46-c0a9-41f5-a4d8-43d3ecd8b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With DataFrames\n",
    "\n",
    "input_catal = './ItemsCatalog.txt'\n",
    "input_sales = './Sales.txt'\n",
    "\n",
    "out_1 = 'outputFolderEx1'\n",
    "\n",
    "# Sales DF\n",
    "in_sales_DF = spark.read.load(input_sales, format='csv', header=False)\\\n",
    "                    .toDF('timestamp', 'userID', 'itemID', 'salePrice')\n",
    "\n",
    "# Catalog DF - only keep useful columns\n",
    "in_cat_DF = spark.read.load(input_catal, format='csv', header=False)\\\n",
    "                    .toDF('itemID', 'name', 'category', 'firstTimeIn', 'suggPrice')\\\n",
    "                    .select('itemID', 'suggPrice')\n",
    "\n",
    "# UDF for getting the year\n",
    "spark.udf.register('getYear', lambda ts: int(ts.split('/')[0]))\n",
    "\n",
    "# Keep sales from 2021 only\n",
    "sales_2021_DF = in_sales_DF.filter('getYear(timestamp) == 2021')\n",
    "\n",
    "# Join 2021 sales with catalog to get suggested price\n",
    "both_prices_DF = sales_2021_DF.join(in_cat_DF, sales_2021_DF.itemID == in_cat_DF.itemID)\n",
    "\n",
    "both_prices_DF.createOrReplaceTempView('my_table')\n",
    "\n",
    "# Directly obtain the DF containing the ratio between the number of sales\n",
    "# at more than the suggeted price and the total sales\n",
    "out_DF = spark.sql(\"\"\"\n",
    "            SELECT itemID, sum(CAST(salePrice > suggPrice AS int))/count(*) AS fract_more\n",
    "            FROM my_table\n",
    "            GROUP BY itemID\n",
    "\"\"\").filter('fract_more > 0.8')\n",
    "\n",
    "out_DF.select('itemID').write.csv(out_1, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad746f2a-cbf1-4b57-9895-52ca5b083c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e238a2-1095-4d6e-934d-67b5ca11cfdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72f52b83-2fd0-4557-9ebb-85961b08c530",
   "metadata": {},
   "source": [
    "### Ex 2\n",
    "\n",
    "#### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7ec4b-9641-4fc3-97a0-70908215825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_catal = './ItemsCatalog.txt'\n",
    "input_sales = './Sales.txt'\n",
    "input_cust = './Customers.txt'\n",
    "\n",
    "out_2_1 = 'outFolderEx2_1'\n",
    "out_2_2 = 'outFolderEx2_2'\n",
    "\n",
    "########################################\n",
    "\n",
    "in_sales_DF = spark.read.load(input_sales, format='csv', header=False)\\\n",
    "                    .toDF('timetamp', 'userID', 'itemID', 'salePrice')\n",
    "in_sales_DF.createOrReplaceTempView('sales_table')\n",
    "\n",
    "spark.udf.register('getYear', lambda ts: int(ts.split('/')[0]))\n",
    "\n",
    "# In the output, users such that:\n",
    "# - Sales from 2020\n",
    "# - Number of different items > 100\n",
    "# - Total money spent > 1000\n",
    "out_2_1_DF = spark.sql(\"\"\"\n",
    "            SELECT userID, count(distinct(itemID)) AS n_items, sum(salePrice) AS tot_spent\n",
    "            FROM sales_table\n",
    "            WHERE getYear(timestamp) == 2020\n",
    "            GROUP BY userID\n",
    "            HAVING count(distinct(itemID)) > 100 AND sum(salePrice) > 1000\n",
    "\"\"\").select('userID')\n",
    "\n",
    "out_2_1_DF.write.csv(out_2_1, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ff82e-98c4-473d-8693-49635375f4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d6611f6-1d08-42b4-987e-171d4918b859",
   "metadata": {},
   "source": [
    "#### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e2989b-420b-4f64-ad08-6a16e887864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_2_1_DF.createOrReplaceTempView('good_2020')\n",
    "\n",
    "spark.udf.register('getMonth', lambda ts: int(ts.split('/')[1]))\n",
    "\n",
    "# Start by selecting only sales of 2021 and joining \n",
    "# the filtered sales with the good customers of 2020:\n",
    "\n",
    "all_2021 = spark.sql(\"\"\"\n",
    "            SELECT sal.userID, getMonth(sal.timestamp) AS month\n",
    "            FROM sales_table AS sal, good_2020\n",
    "            WHERE sal.userID == good_2020.userID\n",
    "\"\"\")\n",
    "\n",
    "all_2021.createOrReplaceTempView('t1')\n",
    "\n",
    "########## maybe can transform these two (^ and v) into a single query (not sure)\n",
    "\n",
    "# Select couples (userID, month) having less than 3 sales (requirement for being classified as good customer)\n",
    "good_2021 = spark.sql(\"\"\"\n",
    "            SELECT userID, month, count(*) AS n_sales_mo\n",
    "            FROM t1\n",
    "            GROUP BY userID, month\n",
    "            HAVING count(*) < 3\n",
    "\"\"\")\n",
    "\n",
    "customers_DF = spark.read.load(input_cust, format='csv', header=False)\\\n",
    "                    .toDF('userID', 'name', 'surname', 'birth')\\\n",
    "                    .select('userID', 'surname')\n",
    "\n",
    "# Isolate these customers who have all 12 months left in the previous DF\n",
    "good_customers_2021 = good_2021.groupBy('userID').agg({'*': 'count'})\\\n",
    "                            .withColumnRenamed('count(1)', 'ok_months')\\\n",
    "                            .filter('ok_months == 12').select('userID')\\\n",
    "\n",
    "# Join on the customers DF (with just ID and surname) to get surname\n",
    "good_surname_2021 = good_customers_2021.join(customer_DF, good_customers_2021.userID == customer_DF.userID)\n",
    "\n",
    "good_surname_2021.write.csv(out_2_2, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e05449-6c55-4cab-93b1-055e8c45709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## IT IS POSSIBLE TO MERGE THE 2 QUERIES\n",
    "\n",
    "good_2021 = spark.sql(\"\"\"\n",
    "            SELECT sal.userID, getMonth(sal.timestamp) AS month, count(*) AS n_sales_mo\n",
    "            FROM sales_table AS sal, good_2020\n",
    "            WHERE sal.userID == good_2020.userID\n",
    "            GROUP BY sal.userID, month\n",
    "            HAVING count(*) < 3\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6c244-fcb4-4cc2-aec4-081cd8e66c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084df452-2efc-4d1a-97e3-e3de7d21ed11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5adad14-e0de-4b4b-9ed1-805735fb7c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01701b2-a206-46e6-811d-45bc2b3c9046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e4cf0-81ce-41e8-aa1e-0fc964e28214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7381f-dd28-4f22-a4c4-1df6ea066612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7bf6fc-bc75-441f-9ec5-2dddb1a56cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022e040-c273-4d58-af16-1897f08cd27a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9511fb-c004-4e7b-8aef-2f470b519d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179663a-5a2a-4212-b1a0-b3911a715dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d60ad3-89d1-48cc-a46f-4ff80188b04e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
