{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1d676ea-1bf8-4267-b88a-2ead56419f2d",
   "metadata": {},
   "source": [
    "# 2020/02/07 - Exam\n",
    "\n",
    "## Part 1 - MCQ\n",
    "\n",
    "1. The false option is b. The column 'features' actually contains the vectors of features of the data set.\n",
    "2. The blocks needed are 5. `data2019.txt` needs 2 blocks, `data2018.txt` needs 1 and `data2017.txt` needs 2.\n",
    "\n",
    "## Part 2 - Programs\n",
    "\n",
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8df62aae-3e60-415e-a430-6b74f5263fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/31 15:12:21 WARN analysis.SimpleFunctionRegistry: The function is2019 replaced a previously registered function.\n",
      "23/01/31 15:12:21 WARN analysis.SimpleFunctionRegistry: The function translpurch replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "# With DFs\n",
    "\n",
    "import sys\n",
    "\n",
    "# out_path = sys.argv[1]\n",
    "# in_path = sys.argv[2]\n",
    "\n",
    "out_path = '/user/s315054/exam_sim/example_7Feb/ex1_DF'\n",
    "in_path = '/data/students/bigdata_internet/exam_examples_data/exam20200207_data/Visualizations_Purchases.txt'\n",
    "\n",
    "in_DF = spark.read.load(in_path, format='csv', sep=',', header=False, inferSchema=True)\\\n",
    "                .toDF('timestamp', 'username', 'itemID', 'purchased')\n",
    "\n",
    "in_DF.createOrReplaceTempView('first_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "654db20a-b5a4-42c8-9e7c-ec97049609a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/31 15:15:41 WARN analysis.SimpleFunctionRegistry: The function is2019 replaced a previously registered function.\n",
      "23/01/31 15:15:41 WARN analysis.SimpleFunctionRegistry: The function translpurch replaced a previously registered function.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Filter for 2019 only\n",
    "# Timestamp format: %Y/%m/%d-%h:%M:%S\n",
    "spark.udf.register('is2019', lambda ts: ts.startswith('2019'))\n",
    "\n",
    "def translPurch(item):\n",
    "    if item == True:\n",
    "        return 1\n",
    "    elif item == False:\n",
    "        return 0\n",
    "\n",
    "spark.udf.register('translPurch', translPurch)\n",
    "\n",
    "tmp1_DF = spark.sql(\"\"\"\n",
    "                SELECT username, count(*) AS n_views, sum(CAST(purchased == True AS int)) AS n_purch\n",
    "                FROM first_table\n",
    "                WHERE is2019(timestamp) == True\n",
    "                GROUP BY username\n",
    "\"\"\")\n",
    "\n",
    "out1_DF = tmp1_DF.filter('n_views > 50').selectExpr('username', 'n_purch/n_views AS purch_rate').filter('purch_rate > 0.0005')\n",
    "\n",
    "out1_DF.write.csv(out_path, header=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9590bf61-5194-4c6d-ad79-55b904d0fc34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 16:05:43 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s315054/exam_sim/example_7Feb/ex1_DF' to trash at: hdfs://BigDataHA/user/s315054/.Trash/Current/user/s315054/exam_sim/example_7Feb/ex1_DF\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/s315054/exam_sim/example_7Feb/ex1_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7f2ec811-cc98-408a-8df6-32973464ed74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Using SparkSQL methods\n",
    "\n",
    "out1_DF = in_DF.filter('is2019(timestamp) == True').withColumn('purchased', in_DF.purchased.cast('int'))\\\n",
    "            .groupBy('username').agg({'*':'count', 'purchased':'sum'}).withColumnRenamed('count(1)', 'n_views')\\\n",
    "            .withColumnRenamed('sum(purchased)', 'n_purchases').filter('n_views > 50')\\\n",
    "            .selectExpr('username', 'n_purchases/n_views AS purch_rate').filter('purch_rate > 0.0005')\n",
    "\n",
    "out1_DF.write.csv(out_path, header=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e53737b6-0a53-4dd2-b114-2e8cf27646fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|username|         purch_rate|\n",
      "+--------+-------------------+\n",
      "|  User30|0.38461538461538464|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out1_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4327856-a203-4790-ac84-1f2089763d8a",
   "metadata": {},
   "source": [
    "### Ex 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c4f99f6b-3210-44c5-8bdc-62fc7d419a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `out_2a': No such file or directory\n",
      "rm: `=': No such file or directory\n",
      "23/01/31 16:52:44 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s315054/exam_sim/example_7Feb/ex2a_DF' to trash at: hdfs://BigDataHA/user/s315054/.Trash/Current/user/s315054/exam_sim/example_7Feb/ex2a_DF\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r out_2a = /user/s315054/exam_sim/example_7Feb/ex2a_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b58134c3-c34e-456d-99ba-27f5d115ba18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/31 16:52:45 WARN analysis.SimpleFunctionRegistry: The function getyear replaced a previously registered function.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# DF\n",
    "in_file = '/data/students/bigdata_internet/exam_examples_data/exam20200207_data/Visualizations_Purchases.txt'\n",
    "out_2a = '/user/s315054/exam_sim/example_7Feb/ex2a_DF'\n",
    "\n",
    "in_DF = spark.read.load(in_path, format='csv', sep=',', header=False, inferSchema=True)\\\n",
    "                .toDF('timestamp', 'username', 'itemID', 'purchased')\n",
    "\n",
    "in_DF.createOrReplaceTempView('start_table')\n",
    "\n",
    "spark.udf.register('getYear', lambda ts: int(ts.split('/')[0]))\n",
    "\n",
    "filt_DF = spark.sql(\"\"\"\n",
    "                SELECT itemID\n",
    "                FROM start_table\n",
    "                WHERE getYear(timestamp) == 2019 OR getYear(timestamp) == 2018\n",
    "                GROUP BY itemID\n",
    "                HAVING sum(CAST(getYear(timestamp) == 2019 AS int)) >= 20  AND sum(CAST(getYear(timestamp) == 2018 AS int)) == 0\n",
    "\"\"\")\n",
    "\n",
    "filt_DF.write.csv(out_2a, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "067f89de-6203-483b-a1ae-d92147c81458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|itemID|\n",
      "+------+\n",
      "|  ID28|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filt_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6a09c9bd-0d77-43ca-afa5-3b1707b283b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|itemID|n_2019|n_2018|\n",
      "+------+------+------+\n",
      "|  ID62|     1|     0|\n",
      "|  ID25|     2|     0|\n",
      "|  ID34|     2|     0|\n",
      "|  ID54|     1|     0|\n",
      "|   ID1|     1|     0|\n",
      "|  ID84|     1|     0|\n",
      "|  ID23|     1|     0|\n",
      "|  ID58|     1|     0|\n",
      "|  ID77|     1|     0|\n",
      "|  ID35|     1|     0|\n",
      "|  ID70|     1|     1|\n",
      "|  ID43|     1|     0|\n",
      "|  ID89|     1|     0|\n",
      "|  ID11|     2|     0|\n",
      "|  ID65|     1|     0|\n",
      "|  ID28|    23|     0|\n",
      "|  ID56|     1|     0|\n",
      "|  ID20|    20|     1|\n",
      "|  ID72|     1|     0|\n",
      "|  ID10|     1|     0|\n",
      "+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check results\n",
    "spark.sql(\"\"\"\n",
    "            SELECT itemID, sum(CAST(getYear(timestamp) == 2019 AS int)) AS n_2019, sum(CAST(getYear(timestamp) == 2018 AS int)) AS n_2018\n",
    "            FROM start_table\n",
    "            GROUP BY itemID\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8485d-12e8-418c-b912-e8c11984271e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bf54afe-e273-43fa-aaf8-7fabbcc81116",
   "metadata": {},
   "source": [
    "### Ex 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cb1a23f4-6165-4c7a-8edb-18a1d8386b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/31 16:57:15 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s315054/exam_sim/example_7Feb/ex2b_DF' to trash at: hdfs://BigDataHA/user/s315054/.Trash/Current/user/s315054/exam_sim/example_7Feb/ex2b_DF1675184235486\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/s315054/exam_sim/example_7Feb/ex2b_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e6903574-d58c-495f-afea-44372b4e0cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "out_2b = '/user/s315054/exam_sim/example_7Feb/ex2b_DF'\n",
    "\n",
    "# Number of purchases of each item\n",
    "n_purch_DF = spark.sql(\"\"\"\n",
    "        SELECT itemID, sum(CAST(purchased AS int)) AS n_purch\n",
    "        FROM start_table\n",
    "        WHERE getYear(timestamp) == 2019\n",
    "        GROUP BY itemID\n",
    "\"\"\")\n",
    "\n",
    "n_purch_DF.createOrReplaceTempView('n_purch_tab')\n",
    "\n",
    "max_purch_DF = spark.sql(\"\"\"\n",
    "        SELECT max(n_purch) AS maxTimesPurch\n",
    "        FROM n_purch_tab\n",
    "\"\"\")\n",
    "\n",
    "max_purch = max_purch_DF.first().asDict()['maxTimesPurch']\n",
    "\n",
    "# Open other path\n",
    "in_path_2 = '/data/students/bigdata_internet/exam_examples_data/exam20200207_data/Items.txt'\n",
    "items_DF = spark.read.load(in_path_2, format='csv', sep=',', header=False, inferSchema=True)\\\n",
    "                .toDF('itemID', 'name', 'ShortDesc', 'Price', 'webPage')\\\n",
    "                .select('itemID', 'name')\n",
    "\n",
    "print('helo')\n",
    "\n",
    "items_DF.createOrReplaceTempView('items_table')\n",
    "\n",
    "max_items_DF = spark.sql(f\"\\\n",
    "                SELECT n_purch_tab.itemID, n_purch_tab.n_purch, items_table.name\\\n",
    "                FROM n_purch_tab, items_table\\\n",
    "                WHERE n_purch_tab.n_purch == {max_purch} AND n_purch_tab.itemID == items_table.itemID\\\n",
    "        \")\n",
    "\n",
    "max_items_DF.select('name', 'n_purch').write.csv(out_2b, sep=',', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ea5ecc4e-20b3-4e69-8665-b3d8eda0c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------+\n",
      "|itemID|n_purch|    name|\n",
      "+------+-------+--------+\n",
      "|  ID20|      5|NameID20|\n",
      "+------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_items_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "df9dc04b-a389-4ab4-a129-90a3078f2c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|itemID|n_purch|\n",
      "+------+-------+\n",
      "|  ID62|      0|\n",
      "|  ID25|      1|\n",
      "|  ID34|      1|\n",
      "|  ID54|      0|\n",
      "|   ID1|      0|\n",
      "|  ID84|      1|\n",
      "|  ID23|      0|\n",
      "|  ID58|      0|\n",
      "|  ID77|      0|\n",
      "|  ID35|      0|\n",
      "|  ID70|      0|\n",
      "|  ID43|      0|\n",
      "|  ID89|      0|\n",
      "|  ID11|      1|\n",
      "|  ID65|      0|\n",
      "|  ID28|      0|\n",
      "|  ID56|      1|\n",
      "|  ID20|      5|\n",
      "|  ID72|      1|\n",
      "|  ID10|      0|\n",
      "+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_purch_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2974206-a4fb-4dad-b66f-bc0065e73e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba6e2d-adbd-4fb2-aca3-f9b24e97545b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
