{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13deb748-458a-4420-98cb-943473f00132",
   "metadata": {},
   "source": [
    "# GraphFrames example - 01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674b55eb-5c1a-41a8-b13d-812bde65364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90a2393-790f-4c29-92bd-3b08e654cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = spark.createDataFrame([\n",
    "\t(\"a\", \"Alice\", 34), \n",
    "\t(\"b\", \"Bob\", 36), \n",
    "\t(\"c\", \"Charlie\", 30), \n",
    "\t(\"d\", \"David\", 29), \n",
    "\t(\"e\", \"Esther\", 32), \n",
    "\t(\"f\", \"Fanny\", 36), \n",
    "\t(\"g\", \"Gabby\", 60) \n",
    "], [\"id\", \"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdfaf2fb-cc7a-4162-a930-d268fc17f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = spark.createDataFrame([ \n",
    "\t(\"a\", \"b\", \"friend\"), \n",
    "\t(\"b\", \"c\", \"follow\"), \n",
    "\t(\"c\", \"b\", \"follow\"), \n",
    "\t(\"f\", \"c\", \"follow\"), \n",
    "\t(\"e\", \"f\", \"follow\"), \n",
    "\t(\"e\", \"d\", \"friend\"), \n",
    "\t(\"d\", \"a\", \"friend\"), \n",
    "\t(\"a\", \"e\", \"friend\") \n",
    "], [\"src\", \"dst\", \"relationship\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdcdec86-1014-421d-9ca2-00883bdb322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = GraphFrame(v, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0908bc24-d55f-4748-a043-4bf57ff40e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  a|  Alice| 34|\n",
      "|  b|    Bob| 36|\n",
      "|  c|Charlie| 30|\n",
      "|  d|  David| 29|\n",
      "|  e| Esther| 32|\n",
      "|  f|  Fanny| 36|\n",
      "|  g|  Gabby| 60|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g1.vertices.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "843add6c-95aa-450f-b4c9-8c66cf0693ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------+\n",
      "|src|dst|relationship|\n",
      "+---+---+------------+\n",
      "|  a|  b|      friend|\n",
      "|  b|  c|      follow|\n",
      "|  c|  b|      follow|\n",
      "|  f|  c|      follow|\n",
      "|  e|  f|      follow|\n",
      "|  e|  d|      friend|\n",
      "|  d|  a|      friend|\n",
      "|  a|  e|      friend|\n",
      "+---+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g1.edges.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9276af77-9549-4b2a-8d45-23ede2d52ac8",
   "metadata": {},
   "source": [
    "## Find n. of edges and n. of vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8e210cc-8b69-4019-b64a-3750251a18a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges: 8\n",
      "Number of vertices: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of edges: {g1.edges.count()}\")\n",
    "print(f\"Number of vertices: {g1.vertices.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e5397-81dc-460c-8a71-86b6668acdf0",
   "metadata": {},
   "source": [
    "## Find lowest age among the vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6c2bbd9-1c4e-4e44-b0e3-a98390d24313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  d|David| 29|\n",
      "+---+-----+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g1.vertices.sort(\"age\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb0197c-16f2-46c3-b8dc-4b5ff4351030",
   "metadata": {},
   "source": [
    "## Count number of \"follow\" edges in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02ddc51e-26c1-4e06-915a-6a6d08d8a01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'follow' edges: 4\n"
     ]
    }
   ],
   "source": [
    "n_follow = g1.edges.filter(\"relationship == 'follow'\").count()\n",
    "print(f\"Number of 'follow' edges: {n_follow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9429a4cb-2c36-4e06-b32e-b19aa1caa831",
   "metadata": {},
   "source": [
    "## *Motif finding* - find all users which follow each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d4e17bf-2a9c-4ae4-a4ce-d4c0076e0bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:=======================================================>(74 + 1) / 75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n",
      "|               a|               b|\n",
      "+----------------+----------------+\n",
      "|[c, Charlie, 30]|    [b, Bob, 36]|\n",
      "|    [b, Bob, 36]|[c, Charlie, 30]|\n",
      "+----------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "g1.find(\"(a)-[]->(b); (b)-[]->(a)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb7bc2-30fc-4637-9b61-6adfe1a9eb7e",
   "metadata": {},
   "source": [
    "## **!!** ***Motif finding* - find chains of 4 users in which at least 2 of the 3 connections are 'friend'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbafad55-e20b-4991-8975-53f522131eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------------+--------------+----------------+--------------+----------------+\n",
      "|              v1|            e1|              v2|            e2|              v3|            e3|              v4|\n",
      "+----------------+--------------+----------------+--------------+----------------+--------------+----------------+\n",
      "|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|[e, f, follow]|  [f, Fanny, 36]|\n",
      "| [e, Esther, 32]|[e, d, friend]|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|\n",
      "|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|[e, d, friend]|  [d, David, 29]|\n",
      "|  [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|[e, f, follow]|  [f, Fanny, 36]|[f, c, follow]|[c, Charlie, 30]|\n",
      "|  [f, Fanny, 36]|[f, c, follow]|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|\n",
      "|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|\n",
      "|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|[a, b, friend]|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|\n",
      "| [e, Esther, 32]|[e, f, follow]|  [f, Fanny, 36]|[f, c, follow]|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|\n",
      "|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|\n",
      "|  [a, Alice, 34]|[a, b, friend]|    [b, Bob, 36]|[b, c, follow]|[c, Charlie, 30]|[c, b, follow]|    [b, Bob, 36]|\n",
      "| [e, Esther, 32]|[e, d, friend]|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|[a, b, friend]|    [b, Bob, 36]|\n",
      "|  [a, Alice, 34]|[a, e, friend]| [e, Esther, 32]|[e, d, friend]|  [d, David, 29]|[d, a, friend]|  [a, Alice, 34]|\n",
      "+----------------+--------------+----------------+--------------+----------------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fourInARow = g1.find(\"(v1)-[e1]->(v2); (v2)-[e2]->(v3); (v3)-[e3]->(v4)\")\n",
    "fourInARow.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63cae8a1-6f1c-4ae9-b15a-a2938f4bdf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/17 10:00:56 WARN analysis.SimpleFunctionRegistry: The function cond replaced a previously registered function.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`fourInARow.e1`' given input columns: [e2, v4, v3, v1, e3, e1, v2]; line 1 pos 5;\\n'Filter 'cond('fourInARow.e1, 'fourInARow.e2, 'fourInARow.e3)\\n+- Project [v1#121, e1#119, v2#123, e2#144, v3#146, e3#179, v4#181]\\n   +- Join Inner, (e3#179.dst = v4#181.id)\\n      :- Join Inner, (e3#179.src = v3#146.id)\\n      :  :- Join Inner, (e2#144.dst = v3#146.id)\\n      :  :  :- Join Inner, (e2#144.src = v2#123.id)\\n      :  :  :  :- Join Inner, (e1#119.dst = v2#123.id)\\n      :  :  :  :  :- Join Inner, (e1#119.src = v1#121.id)\\n      :  :  :  :  :  :- Project [named_struct(src, src#6, dst, dst#7, relationship, relationship#8) AS e1#119]\\n      :  :  :  :  :  :  +- LogicalRDD [src#6, dst#7, relationship#8], false\\n      :  :  :  :  :  +- Project [named_struct(id, id#0, name, name#1, age, age#2L) AS v1#121]\\n      :  :  :  :  :     +- LogicalRDD [id#0, name#1, age#2L], false\\n      :  :  :  :  +- Project [named_struct(id, id#0, name, name#1, age, age#2L) AS v2#123]\\n      :  :  :  :     +- LogicalRDD [id#0, name#1, age#2L], false\\n      :  :  :  +- Project [named_struct(src, src#6, dst, dst#7, relationship, relationship#8) AS e2#144]\\n      :  :  :     +- LogicalRDD [src#6, dst#7, relationship#8], false\\n      :  :  +- Project [named_struct(id, id#0, name, name#1, age, age#2L) AS v3#146]\\n      :  :     +- LogicalRDD [id#0, name#1, age#2L], false\\n      :  +- Project [named_struct(src, src#6, dst, dst#7, relationship, relationship#8) AS e3#179]\\n      :     +- LogicalRDD [src#6, dst#7, relationship#8], false\\n      +- Project [named_struct(id, id#0, name, name#1, age, age#2L) AS v4#181]\\n         +- LogicalRDD [id#0, name#1, age#2L], false\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o102.filter.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`fourInARow.e1`' given input columns: [e2, v4, v3, v1, e3, e1, v2]; line 1 pos 5;\n'Filter 'cond('fourInARow.e1, 'fourInARow.e2, 'fourInARow.e3)\n+- Project [v1#121, e1#119, v2#123, e2#144, v3#146, e3#179, v4#181]\n   +- Join Inner, (e3#179.dst = v4#181.id)\n      :- Join Inner, (e3#179.src = v3#146.id)\n      :  :- Join Inner, (e2#144.dst = v3#146.id)\n      :  :  :- Join Inner, (e2#144.src = v2#123.id)\n      :  :  :  :- Join Inner, (e1#119.dst = v2#123.id)\n      :  :  :  :  :- Join Inner, (e1#119.src = v1#121.id)\n      :  :  :  :  :  :- Project [named_struct(src, src#6, dst, dst#7, relationship, relationship#8) AS e1#119]\n      :  :  :  :  :  :  +- LogicalRDD [src#6, dst#7, relationship#8], false\n      :  :  :  :  :  +- Project [named_struct(id, id#0, name, name#1, age, age#2L) AS v1#121]\n      :  :  :  :  :     +- LogicalRDD [id#0, name#1, age#2L], false\n      :  :  :  :  +- Project [named_struct(id, id#0, name, name#1, age, age#2L) AS v2#123]\n      :  :  :  :     +- LogicalRDD [id#0, name#1, age#2L], false\n      :  :  :  +- Project [named_struct(src, src#6, dst, dst#7, relationship, relationship#8) AS e2#144]\n      :  :  :     +- LogicalRDD [src#6, dst#7, relationship#8], false\n      :  :  +- Project [named_struct(id, id#0, name, name#1, age, age#2L) AS v3#146]\n      :  :     +- LogicalRDD [id#0, name#1, age#2L], false\n      :  +- Project [named_struct(src, src#6, dst, dst#7, relationship, relationship#8) AS e3#179]\n      :     +- LogicalRDD [src#6, dst#7, relationship#8], false\n      +- Project [named_struct(id, id#0, name, name#1, age, age#2L) AS v4#181]\n         +- LogicalRDD [id#0, name#1, age#2L], false\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$2(TreeNode.scala:295)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$13.apply(TreeNode.scala:354)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:354)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3411)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1484)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1498)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_308/1948268342.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cond'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond2Of3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBooleanType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mfourInARow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cond(fourInARow.e1, fourInARow.e2, fourInARow.e3)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \"\"\"\n\u001b[1;32m   1357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`fourInARow.e1`' given input columns: [e2, v4, v3, v1, e3, e1, v2]; line 1 pos 5;\\n'Filter 'cond('fourInARow.e1, 'fourInARow.e2, 'fourInARow.e3)\\n+- Project [v1#121, e1#119, v2#123, e2#144, v3#146, e3#179, v4#181]\\n   +- Join Inner, (e3#179.dst = v4#181.id)\\n      :- Join Inner, (e3#179.src = v3#146.id)\\n      :  :- Join Inner, (e2#144.dst = v3#146.id)\\n      :  :  :- Join Inner, (e2#144.src = v2#123.id)\\n      :  :  :  :- Join Inner, (e1#119.dst = v2#123.id)\\n      :  :  :  :  :- Join Inner, (e1#119.src = v1#121.id)\\n      :  :  :  :  :  :- Project [named_struct(src, src#6, dst, dst#7, relationship, relationship#8) AS e1#119]\\n      :  :  :  :  :  :  +- LogicalRDD [src#6, dst#7, relationship#8], false\\n      :  :  :  :  :  +- Project [named_struct(id, id#0, name, name#1, age, age#2L) AS v1#121]\\n      :  :  :  :  :     +- LogicalRDD [id#0, name#1, age#2L], false\\n      :  :  :  :  +- Project [named_struct(id, id#0, name, name#1, age, age#2L) AS v2#123]\\n      :  :  :  :     +- LogicalRDD [id#0, name#1, age#2L], false\\n      :  :  :  +- Project [named_struct(src, src#6, dst, dst#7, relationship, relationship#8) AS e2#144]\\n      :  :  :     +- LogicalRDD [src#6, dst#7, relationship#8], false\\n      :  :  +- Project [named_struct(id, id#0, name, name#1, age, age#2L) AS v3#146]\\n      :  :     +- LogicalRDD [id#0, name#1, age#2L], false\\n      :  +- Project [named_struct(src, src#6, dst, dst#7, relationship, relationship#8) AS e3#179]\\n      :     +- LogicalRDD [src#6, dst#7, relationship#8], false\\n      +- Project [named_struct(id, id#0, name, name#1, age, age#2L) AS v4#181]\\n         +- LogicalRDD [id#0, name#1, age#2L], false\\n\""
     ]
    }
   ],
   "source": [
    "def cond2Of3(e1, e2, e3):\n",
    "    a = (e1.relationship == \"friend\")\n",
    "    b = (e2.relationship == \"friend\")\n",
    "    c = (e3.relationship == \"friend\")\n",
    "    \n",
    "    return (int(a)+int(b)+int(c) >= 2)\n",
    "\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "#udf(cond2Of3, BooleanType())\n",
    "spark.udf.register('cond', cond2Of3, BooleanType())\n",
    "\n",
    "fourInARow.filter('cond(fourInARow.e1, fourInARow.e2, fourInARow.e3)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b9132-e90e-48d9-a2f9-29cb5c585971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ccdfd-5634-4a07-8723-fbd1dc5a2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Why won't this work???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a94ef-7db8-4ddc-9e77-205cd7e5ca18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a236282c-49e1-4348-932e-ea5711fc1f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a1b2b-f2b3-4ea6-8b3e-ad81ae20bd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8cbd7-ce84-44be-b547-10636cc4c7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GraphFrames (Yarn)",
   "language": "python",
   "name": "graphframe_yarn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
