{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2d04ac-65ba-4e41-a7ff-423da14b745c",
   "metadata": {},
   "source": [
    "# Laboratory 03\n",
    "\n",
    "Using Spark SQL to analyze historical data about the usage of bike sharing in Barcelona."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0876c98-f61a-408b-977c-bce9b1d2c419",
   "metadata": {},
   "source": [
    "## Using RDDs\n",
    "\n",
    "First, the exercises will be solved using RDDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed8665f-fba7-49ca-84d6-fb2c86ab4815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 11:14:36 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s315054/lab03/output_RDD.csv' to trash at: hdfs://BigDataHA/user/s315054/.Trash/Current/user/s315054/lab03/output_RDD.csv\n"
     ]
    }
   ],
   "source": [
    "# Clear the destination folder to prevent overwriting\n",
    "!hdfs dfs -rm -r /user/s315054/lab03/output_RDD.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d01523e-f58e-4f16-a4b4-4a40e86e9cd7",
   "metadata": {},
   "source": [
    "### 1. Input files\n",
    "\n",
    "* `register.csv` contains info about the used/free bike parking slots in approx. 3000 stations. It contains the header. It is needed to filter out errors (used slots = 0, free slots = 0); format: `station\\ttimestamp\\tused_slots\\tfree_slots`\n",
    "* `stations.csv` contains the description of the stations; format: `id\\tlongitude\\tlatitude\\tname`\n",
    "    \n",
    "We need to first remove the header (1st line of CSV files) and separate the elements in each line. \n",
    "Then, on the first file we need to remove \"wrong\" lines, i.e., these lines in which both the used slots and the free slots of the station are set to 0 (errors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297d3833-ff2f-4917-b135-340a46e678a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "path_register = \"/data/students/bigdata_internet/lab3/register.csv\"\n",
    "path_stations = \"/data/students/bigdata_internet/lab3/stations.csv\"\n",
    "# out_path = '/user/s315054/lab03/output_RDD.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "916f722d-f240-4648-8d68-6405f6bb6506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File 1\n",
    "RDD_reg_file = sc.textFile(path_register)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686cc40-48c9-4525-8c43-90fc70490ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 1st line (header)\n",
    "header_reg = RDD_reg_file.first()\n",
    "RDD_reg_noHead = RDD_reg_file.filter(lambda l: l != header_reg)\n",
    "# Output was removed because it contained logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cbc7299-9131-468c-a1e1-f2e47b872148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate elements at (`\\t`)\n",
    "RDD_reg = RDD_reg_noHead.map(lambda line: line.split('\\t'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeb1c0b-ce68-47e7-8640-b6c75cc5af18",
   "metadata": {},
   "source": [
    "#### 1.1 - Count the rows befor and after removing the wrong elements\n",
    "* Before filtering: 25319028 rows\n",
    "* After filtering: 25104121 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b70a41-7f05-4d2a-af93-fd1ab0cd3dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines before filtering is: 25319028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines after filtering is: 25104121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Remove wrong lines, i.e., the ones containing\n",
    "RDD_reg_correct = RDD_reg\\\n",
    "    .filter(lambda line: (int(line[2]) != 0) or (int(line[3]) != 0))\n",
    "\n",
    "lines_before = RDD_reg.count()\n",
    "print(f\"The number of lines before filtering is: {lines_before}\")\n",
    "lines_after = RDD_reg_correct.count()\n",
    "print(f\"The number of lines after filtering is: {lines_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff09578-b484-4a11-81a6-148ea47e75ee",
   "metadata": {},
   "source": [
    "#### 1.2 - Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0439318d-78af-4f71-8b86-31e7f3acfc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening file 2\n",
    "RDD_stat_file = sc.textFile(path_stations)\n",
    "\n",
    "# Remove 1st line (header)\n",
    "header_stat = RDD_stat_file.first()\n",
    "RDD_stat_noHead = RDD_stat_file.filter(lambda l: l != header_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6522dc4-699e-48ce-a366-cd43e7f4793c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines in the stations file is 3301\n"
     ]
    }
   ],
   "source": [
    "# Separate elements\n",
    "RDD_stat = RDD_stat_noHead.map(lambda line: line.split('\\t'))\n",
    "n_stations = RDD_stat.count()\n",
    "print(f\"The number of lines in the stations file is {n_stations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f5121e-9665-4977-a63f-82c9e863bcb1",
   "metadata": {},
   "source": [
    "### 2. Exercise\n",
    "\n",
    "From the RDD of the 1st file, create a pair RDD containing as key the tuple `(timestamp, station_ID)` and as value the number of free slots. Notice that, if some entries have the same day and hour, regardless of the month and minute, they will have the same key. This is crucial for the next steps.\n",
    "Then, having defined the function `critCount()`, which assigns a tuple `(key, [+1, +1])` to critical RDD elements and `(key, [0, +1])` to others, it is possible to apply a `reduceByKey()` transformation to sum the corresponding elements of the lists and obtain, for each key: `[number_of_critical_measurements, number_of_measurements]`. These two values are the ones that need to be divided for obtaining the criticality of the corresponding station and time slot pair.\n",
    "\n",
    "The next operation consists in dividing the first element of each list by the second one to get the criticality as values in a new pair RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249c44f3-5bc3-45df-aac3-a9a2d35a08b6",
   "metadata": {},
   "source": [
    "#### 2.1 - evaluate criticality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25de8b60-175d-4168-88f9-fdc1e0f552bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate pairs (timestamp, id)\n",
    "RDD_pair_all = RDD_reg_correct\\\n",
    ".map(lambda l: ((tuple(\n",
    "    datetime.strptime(l[1], \"%Y-%m-%d %H:%M:%S\")\\\n",
    "    .strftime(\"%A %H\")\\\n",
    "    .split(' ')), l[0]), int(l[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44bc8075-e560-4959-b986-6dc3db200b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def critCount(pair):\n",
    "    if pair[1] == 0:\n",
    "        return (pair[0], [+1, +1])\n",
    "    else:\n",
    "        return (pair[0], [0, +1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81d170f6-0457-4b4a-84ba-4f2d3f56bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count critical for each key ((date, day), ID) and total elements\n",
    "# Assign +1 to elements whose value is 0 and 0 to the ones whose \n",
    "# value is not; after this, sum the values by using reduceByKey\n",
    "RDD_n_crit = RDD_pair_all.map(lambda pair: critCount(pair))\\\n",
    ".reduceByKey(lambda v1, v2: [v1[0]+v2[0], v1[1]+v2[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af65e154-6351-4d0a-90ad-948ff5378f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of distinct (station, timeslot) pairs is: 47550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"The number of distinct (station, timeslot) pairs is: {RDD_n_crit.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "151d176f-5716-4b50-8813-d2038984f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate criticality:\n",
    "RDD_criticality = RDD_n_crit\\\n",
    ".map(lambda pair: (pair[0], float(pair[1][0])/float(pair[1][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571347c9-401a-4f9b-9383-988575cd80c4",
   "metadata": {},
   "source": [
    "#### 2.2 - Find elements above a certain threshold\n",
    "\n",
    "Let the user decide the threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03d27be9-203e-4ebe-9010-08c29b052966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the threshold:  0.6\n"
     ]
    }
   ],
   "source": [
    "# Define the threshold\n",
    "val_is_good = False\n",
    "while (not val_is_good):\n",
    "    threshold = float(input(\"Enter the threshold: \"))\n",
    "    if (threshold >= 0.0) and (threshold <= 1.0):  # Check for validity\n",
    "        val_is_good = True        \n",
    "    else:\n",
    "        print(\"Invalid threshold: the value needs to lay between 0 and 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3df4cb7e-052d-484f-b9f1-5ba007ab6c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate elements whose criticality is above the threshold\n",
    "RDD_thresh = RDD_criticality.filter(lambda pair: pair[1]>=threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a99f0f-8660-4112-a765-63cce7850553",
   "metadata": {},
   "source": [
    "#### 2.3 - Order by increasing criticality\n",
    "\n",
    "If criticality is the same, use ID; if also ID is the same, use the day of the week (Monday < Tuesday < ... < Sunday) and then the hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f765483-4686-4f25-ab14-997677604e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_sorted = RDD_thresh\\\n",
    ".sortBy(lambda pair: \\\n",
    "        (pair[1], int(pair[0][1]), int(time.strptime(pair[0][0][0], \"%A\")\\\n",
    "                                       .tm_wday), int(pair[0][0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b750826-b74d-462b-a628-b46c93da916d",
   "metadata": {},
   "source": [
    "#### 2.4 - Storing on output file\n",
    "\n",
    "Format: .csv with elements separated by tabs (\\t).\n",
    "\n",
    "First, create a pair RDD containing as key the station ID from the RDD of stations.\n",
    "Then, rearrange the sorted RDD so that the keys are the station IDs and the values are lists: `[(day_of_the_week, hour), criticality]`\n",
    "Having the IDs as keys, it is possible to join them (`join()` transformation) so that each entry of the criticality RDD will be associated with latitude and longitude of the corresponding station\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c964bb86-cae6-4747-98bc-a46d90174713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working on the RDD of stations\n",
    "# RDD_stat_ID will contain: (ID, [latitude, longitude])\n",
    "RDD_stat_ID = RDD_stat.map(lambda row: (row[0], row[1:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c774c459-c44c-4d7f-b1bd-237a2452dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the sorted RDD:\n",
    "# RDD_sorted_ID will contain: (ID, [(Day, Hour), criticality])\n",
    "RDD_sorted_ID = RDD_sorted.map(lambda pair: (pair[0][1], [pair[0][0], pair[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d628a37-be93-409a-b494-b708dd366fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two RDDs\n",
    "RDD_comb = RDD_sorted_ID.join(RDD_stat_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d19b9111-ebb3-44e2-82fc-d434cdbee626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put together RDD to be stored as CSV (actually, TSV):\n",
    "RDD_out = RDD_comb.map(lambda elem: f\"{elem[0]}\\t{elem[1][1][0]}\\t{elem[1][1][1]}\\t{elem[1][0][0][0]}\\t{elem[1][0][0][1]}\\t{elem[1][0][1]}\")\n",
    "# Need to add the header:\n",
    "RDD_header = sc.parallelize([\"station\\tlongitude\\tlatitude\\tday\\thour\\tcriticality\"])\n",
    "# Include the header as 1st element of the RDD\n",
    "RDD_out_header = RDD_header.union(RDD_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edacfd8d-c25e-4d68-abc1-7b8da7d5e239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter output file path:  /user/s315054/lab03/output_RDD.csv\n"
     ]
    }
   ],
   "source": [
    "## The output path needs to be decided by the user:\n",
    "out_path = input(\"Enter output file path: \")\n",
    "## '/user/s315054/lab03/output_RDD.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6584bbe-614c-483f-a1ed-335add0f5d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "RDD_out_header.saveAsTextFile(out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c635dd1-c8b7-488d-9c2d-cd187fea7900",
   "metadata": {},
   "source": [
    "#### 2.5 - Results for threshold = 0.6\n",
    "\n",
    "The output file for $threshold = 0.6$ contains 6 lines, which are displayed 4 cells above.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cf7a9c-dbc0-471b-b772-13babc95a18c",
   "metadata": {},
   "source": [
    "## Using SparkSQL\n",
    "\n",
    "The same exercise has also been solved using Spark SQL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d4a752-3b90-4667-b17a-76e37c5e6180",
   "metadata": {},
   "source": [
    "### 1. Import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee6e8e1a-cc2d-4722-924b-91cdc1dbf8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/19 11:17:12 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s315054/lab03/output_DF.csv' to trash at: hdfs://BigDataHA/user/s315054/.Trash/Current/user/s315054/lab03/output_DF.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/s315054/lab03/output_DF.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f403690-f69a-47f8-b292-fbcbfd8b4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "path_register = \"/data/students/bigdata_internet/lab3/register.csv\"\n",
    "path_stations = \"/data/students/bigdata_internet/lab3/stations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98db9635-670f-46de-a03c-73d3a2f1052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "reg_DF = spark.read.option(\"header\",\"true\")\\\n",
    "                .option(\"sep\", \"\\t\")\\\n",
    "                .option(\"multiLine\", \"true\")\\\n",
    "                .option(\"ignoreTrailingWhiteSpace\", \"true\")\\\n",
    "                .csv(path_register)\n",
    "\n",
    "reg_DF.cache()\n",
    "reg_DF.createOrReplaceTempView(\"register\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "555cb5ed-b12e-4799-acb4-928bf76ff649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove wrong lines\n",
    "correct_reg_DF = spark.sql(\"SELECT * \\\n",
    "                            FROM register \\\n",
    "                            WHERE used_slots != 0 OR free_slots != 0 \\\n",
    "                            \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cd600a-7e6d-46a8-990a-9261bf6bfecd",
   "metadata": {},
   "source": [
    "#### 1.1.1 Count the rows before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb4e764c-5f33-468d-9e23-e77c0b47b001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before: 25319028\n",
      "Number of rows after: 25104121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the rows before and after the filtering operation\n",
    "n_before = reg_DF.count()\n",
    "n_after = correct_reg_DF.count()\n",
    "\n",
    "print(f\"Number of rows before: {n_before}\\nNumber of rows after: {n_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a191651-cfd4-4441-80dd-921cfd8a3344",
   "metadata": {},
   "source": [
    "Read second file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e3958c8-d2a3-4a2b-a650-cf49f2ff1757",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_DF = spark.read.option(\"header\",\"true\")\\\n",
    "                    .option(\"sep\", \"\\t\")\\\n",
    "                    .option(\"multiLine\", \"true\")\\\n",
    "                    .option(\"ignoreTrailingWhiteSpace\", \"true\")\\\n",
    "                    .csv(path_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910dbfd0-e28d-48e8-b48b-fc3045f38129",
   "metadata": {},
   "source": [
    "### 2. Criticality evaluation\n",
    "\n",
    "The idea was to first isolate the useful columns (station ID, timeslot and number of free slots), and then work on `free_slots` to understand if the table entry was critical and evaluate the terms needed for the computation of criticality.\n",
    "\n",
    "* The number of critical records for a given station and timeslot was obtained by assigning each record 1 if it was critical and 0 else (`criticality` column); then we could group records according to station ID and timeslot and perform an aggregate sum on `criticality`.\n",
    "* The number of records for each station and timeslot pair was simply obtained by performing a `count()` operations of the grouped rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4f45834-1628-4c94-8bcf-5a8dda573276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(datestring)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to convert the date into a timeslot\n",
    "spark.udf.register(\"timeStamp2timeSlot\",\\\n",
    "                   lambda datestring: \\\n",
    "                   datetime.strptime(datestring, \"%Y-%m-%d %H:%M:%S\")\\\n",
    "                   .strftime(\"%A %H\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d553f68b-d8f7-4032-9e43-90ce5ab48414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[station: string, timeslot: string, critical: double, free_slots: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DF containing station ID, timeslot, \n",
    "# 1 if critical/0 if not and free slots\n",
    "test_DF = correct_reg_DF.selectExpr('station', \\\n",
    "                    'timeStamp2timeSlot(timestamp) AS timeslot', \\\n",
    "                    'CAST(free_slots == 0 AS double) AS critical', \\\n",
    "                    'free_slots')\n",
    "# Speed up calculations:\n",
    "test_DF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b9b1c2a-4a01-4bbf-8945-1982e94bcd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each timeslot and station, get the total critical \n",
    "# records and total n. of records\n",
    "critical_fact_DF = test_DF.groupBy('station', 'timeslot')\\\n",
    "        .agg({'critical':'sum', 'free_slots':'count'})\\\n",
    "        .withColumnRenamed('sum(critical)', 'crit_sum')\\\n",
    "        .withColumnRenamed('count(free_slots)', 'free_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977a21ba-6fc9-4ea7-9bd6-6fe3d15fe6f2",
   "metadata": {},
   "source": [
    "#### 2.1 - Evaluate criticality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea7beca6-183f-4e52-bdf8-1b4b8ac24fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get criticality, simply divide the count of critical\n",
    "# slots per timestamp by the total n. of records\n",
    "critical_DF = critical_fact_DF.selectExpr('CAST(station AS int)',\\\n",
    "                'timeslot', 'crit_sum/free_count AS criticality')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5948d15e-ca3f-4e71-8c90-47c46bca49a9",
   "metadata": {},
   "source": [
    "#### 2.2 Select criticality values > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01cc65df-46d5-4fd0-bde9-0831bab1d33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the threshold:  0.6\n"
     ]
    }
   ],
   "source": [
    "# Define the threshold\n",
    "val_is_good = False\n",
    "while (not val_is_good):\n",
    "    threshold = float(input(\"Enter the threshold: \"))\n",
    "    if (threshold >= 0.0) and (threshold <= 1.0):       # Check for validity\n",
    "        val_is_good = True    \n",
    "    else:\n",
    "        print(\"Invalid threshold: the value needs to lay between 0 and 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "888a9ae7-c574-4251-a26e-fa68c5356b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get records for which the criticality is above threshold\n",
    "thresh_DF = critical_DF.filter(f'criticality > {threshold}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f129035a-4a5a-45b1-b42d-80f449c336e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_DF.createOrReplaceTempView('ordered')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ff190-bf5f-4690-9ede-4094a8e25a52",
   "metadata": {},
   "source": [
    "#### 2.3 Order results in increasing order\n",
    "\n",
    "In order to carry out this task it was needed to create some UDFs for handling the timeslot.\n",
    "\n",
    "* `dayValue()` returns an integer associated with the day of the week of the input timeslot (Monday: 0, Sunday: 6).\n",
    "* `hourValue()` returns the hour of the input tumeslot (as int).\n",
    "* `dayOfWeek()` returns the string containing the day of the week of the input timeslot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db8bcd34-fab9-46f6-8901-ac01b61bdacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/19 11:20:16 WARN analysis.SimpleFunctionRegistry: The function dayofweek replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(ts)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dayValue(ts):\n",
    "    mapping = { \"Monday\":0, \"Tuesday\":1, \n",
    "               \"Wednesday\":2, \"Thursday\":3, \n",
    "               \"Friday\":4, \"Saturday\":5, \"Sunday\":6}\n",
    "    sepLine = ts.split(' ')\n",
    "    return mapping[sepLine[0]]\n",
    "\n",
    "spark.udf.register(\"dayValue\", dayValue)\n",
    "spark.udf.register(\"hourValue\", lambda ts: int(ts.split()[1]))\n",
    "spark.udf.register(\"dayOfWeek\", lambda ts: ts.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20b1ca30-ab46-4b27-8498-f986ce7216c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordering\n",
    "ordered_DF = spark.sql(\"\"\"SELECT * \n",
    "    FROM ordered \n",
    "    ORDER BY criticality, station, \n",
    "            dayValue(timeslot), hourValue(timeslot)\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafa878e-d6db-49dc-a596-62d14fd6ded6",
   "metadata": {},
   "source": [
    "#### 2.4 Store pairs in a csv file inside the HDFS\n",
    "\n",
    "It is again needed to perform the join operation between the DF containing the criticality values and the one containing the station coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8dbfb7c-d109-4ef9-adbf-fa161e201263",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_DF = ordered_DF.join(stations_DF\\\n",
    "    .selectExpr('id AS station', 'longitude', 'latitude'), 'station')\\\n",
    "    .selectExpr('station', 'longitude', 'latitude',\\\n",
    "                'dayOfWeek(timeslot) AS day', \\\n",
    "                'hourValue(timeslot) AS hour', 'criticality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3058340-23d2-48e4-bf31-19421dbd42fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter output file path:  /user/s315054/lab03/output_DF.csv\n"
     ]
    }
   ],
   "source": [
    "## The output path needs to be decided by the user:\n",
    "out_path = input(\"Enter output file path: \")\n",
    "#/user/s315054/lab03/output_DF.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a563457a-bd92-4f51-a3b2-ea880a169aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined_DF.write.csv(out_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a84008-4e26-434a-b982-015da2d17298",
   "metadata": {},
   "source": [
    "#### 2.5 - Results for threshold = 0.6\n",
    "\n",
    "For $threshold = 0.6$, the records obtained are 5. They correspond with the ones obtained in the first part, using RDDs.\n",
    "\n",
    "Above, the final result (content of the file) is shown.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d305aeb-7766-4d16-98b1-92af0744ee94",
   "metadata": {},
   "source": [
    "## Part 3 - Bonus Task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3734cc28-8908-4c92-a9a5-03cbdfe8dbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.centerDist(coords1)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the distance from the city center\n",
    "center_lat = 41.386904\n",
    "center_lon = 2.169989\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def centerDist(coords1):\n",
    "    lat1 = float(coords1[0]) * (np.pi/180)\n",
    "    lon1 = float(coords1[1]) * (np.pi/180)\n",
    "    lat2 = 41.386904 * (np.pi/180)\n",
    "    lon2 = 2.169989 * (np.pi/180)\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = np.sin(dlat/2.0)**2+np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6367 * c\n",
    "    return float(km)\n",
    "\n",
    "spark.udf.register(\"centerDist\", centerDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e0f5539-fb21-4703-a809-3cc0a5f0d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get center distance for each station\n",
    "dist_DF = stations_DF\\\n",
    "    .selectExpr(\"id\", \"centerDist((latitude, longitude)) AS center_dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74800e40-5183-4caf-891a-57bd3ce757ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_reg_DF.createOrReplaceTempView('readings_correct') # from exercise 2\n",
    "# Evaluate average utilization\n",
    "avg_used_DF = spark.sql(\"\"\"\n",
    "    SELECT station, sum(used_slots)/count(used_slots) AS avg_used \n",
    "    FROM readings_correct \n",
    "    GROUP BY station \n",
    "    ORDER BY CAST(station AS int)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05e94fb3-719c-4c0d-a77d-cbf1545927ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with the DF of distances from center\n",
    "avg_and_dist_DF = avg_used_DF.join(dist_DF\\\n",
    "                        .withColumnRenamed(\"id\", \"station\"), \"station\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "74c875e1-531e-4f2b-a809-2c66101111c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_and_dist_DF.createOrReplaceTempView(\"avg_and_dist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3957a43-8b82-4091-8e90-4b45e9ccfdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "U1_DF = avg_and_dist_DF.filter(\"center_dist < 1.5\")\\\n",
    "        .selectExpr(\"sum(avg_used)/count(avg_used) AS U1\")\n",
    "U1 = U1_DF.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ad4fc48-bb22-48a4-ac1a-dc55615cc182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "U2_DF = avg_and_dist_DF.filter(\"center_dist >= 1.5\")\\\n",
    "        .selectExpr(\"sum(avg_used)/count(avg_used) AS U2\")\n",
    "U2 = U2_DF.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c9b7e3e-998f-4ee3-9c1d-46b6e68bbb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of used slots in stations at less than 1.5 km from the center: 8.1756828331052\n",
      "Average number of used slots in stations at more than 1.5 km from center: 7.8692836432461055\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average number of used slots in stations at less than 1.5 km from the center: {U1}\\nAverage number of used slots in stations at more than 1.5 km from center: {U2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6179c01-3a6c-43dd-b13c-bfbae8aff3da",
   "metadata": {},
   "source": [
    "### 3.1 - Where are the most used stations located?\n",
    "\n",
    "The average number of used slots is higher in stations located closer to the city center.\n",
    "Indeed:\n",
    "* $U1 = avg(U(S_i))\\ where\\ S_i :\\ d(S_i, center) < 1.5\\ km = 8.176\\ slots$\n",
    "* $U2 = avg(U(S_i))\\ where\\ S_i :\\ d(S_i, center) \\geq 1.5\\ km = 7.869\\ slots$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
