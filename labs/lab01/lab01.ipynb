{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ddd9c88-8d5b-4c32-9129-e343d6991d19",
   "metadata": {},
   "source": [
    "# Lab 01\n",
    "\n",
    "## Exercise 01\n",
    "\n",
    "Store content of a text file in a RDD, then separate the elements in a new rdd;\n",
    "Eventually, sum all numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006315ea-74cb-46a7-8a8f-7da18019612b",
   "metadata": {},
   "source": [
    "### 1.1 Execute in Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9306f27-c742-419a-b812-9bf427471d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum is: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile(\"/data/students/bigdata_internet/lab1/lab1_dataset.txt\")\n",
    "fields_rdd = rdd.map(lambda line: line.split(\",\")) \n",
    "value_rdd = fields_rdd.map(lambda l: int(l[1]))\n",
    "value_sum = value_rdd.reduce(lambda v1, v2: v1+v2) \n",
    "print(\"The sum is:\", value_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af161688-fa51-46a1-8c1d-6ad1195ecf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['alice', '4'], ['bob', '5'], ['john', '4'], ['alice', '3'], ['john', '8'], ['bob', '3'], ['alice', '7'], ['john', '9'], ['bob', '3']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fieldsList = fields_rdd.collect()\n",
    "print(fieldsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb7698e-42e9-4a86-a098-c75303c4f091",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "\n",
    "1. Printed value: 46 (sum of all numbers - values - in the pairs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df915acb-dc51-4a4d-83ff-9179f27c13b7",
   "metadata": {},
   "source": [
    "2. Lines:\n",
    "\n",
    "    * Line 1: store content of the file `lab1_dataset.txt` into a new RDD\n",
    "    * Line 2: create a new RDD by splitting the elements in the file at \",\"\n",
    "    * Line 3: create a new RDD containing only the numbers (need to cast them as int)\n",
    "    * Line 4: apply the `reduce()` method to sum all numbers and store the result in a local Python variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafb531b-2f78-4e1b-844b-8102f9958eb5",
   "metadata": {},
   "source": [
    "3. No, since the driver program was run locally (on pyspark.polito.it), not on the cluster nodes, therefore it was not stored on the distributed file system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fee3a9b-03f1-42f8-9248-a546da423d4c",
   "metadata": {},
   "source": [
    "4. By changing the kernel to YARN, the program is executed by the cluster nodes. It takes more time to run the program, since it is needed to split the work among the available servers first (automatically done by the YARN scheduler)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdd771f-8ef4-4958-8968-e19df30f4634",
   "metadata": {},
   "source": [
    "5. Now the job is seen at hue, since it was executed by the cluster nodes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9222013e-629b-4e76-8961-c1f8c3ea038b",
   "metadata": {},
   "source": [
    "### 1.2 Execute in a pyspark shell\n",
    "\n",
    "By using `%%bash` the cell is read as a series of terminal commands. The 1st line launches the PySpark interactive shell, while the following ones are executed in that environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e13cae-5fe4-420a-af82-52e7c78df360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum is: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1425774/lib/spark) overrides detected (/opt/cloudera/parcels/CDH/lib/spark).\n",
      "WARNING: Running pyspark from user-defined location.\n",
      "23/01/21 10:01:02 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/01/21 10:01:02 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/01/21 10:01:02 WARN util.Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/01/21 10:01:02 WARN util.Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/01/21 10:01:02 WARN util.Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/01/21 10:01:02 WARN util.Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pyspark --master local --deploy-mode client <<EOF\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName(\"Ex01_2\")\n",
    "sc = SparkContext(conf = conf)\n",
    "rdd = sc.textFile(\"/data/students/bigdata_internet/lab1/lab1_dataset.txt\")\n",
    "fields_rdd = rdd.map(lambda line: line.split(\",\")) \n",
    "value_rdd = fields_rdd.map(lambda l: int(l[1]))\n",
    "value_sum = value_rdd.reduce(lambda v1, v2: v1+v2) \n",
    "print(\"The sum is:\", value_sum)\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f4cee-f7c1-46e6-806d-3ca1628fd5e2",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "1. `--master local` means that the scheduler used is local (the local server did the computation). It is the same as using PySpark (local) as kernel\n",
    "2. `--deploy-mode client` means that the driver program is executed locally (on jupyter.polito.it). In this case, then both the driver and the execution are hosted locally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b4d5db-3505-4035-9472-b00d5acd4d0f",
   "metadata": {},
   "source": [
    "### 1.3 Create a Spark script and run it from the command line\n",
    "\n",
    "The file is stored on the local file system. By using `spark-submit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8d71ccb-4775-445b-ba79-e5fe043f2042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1425774/lib/spark) overrides detected (/opt/cloudera/parcels/CDH/lib/spark).\n",
      "WARNING: Running spark-class from user-defined location.\n",
      "23/01/11 15:11:10 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/01/11 15:11:10 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "The sum is: 46                                                                  \n"
     ]
    }
   ],
   "source": [
    "!spark-submit --master local --deploy-mode client 'lab01_ex1_1.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71422f2d-5277-4fe3-a268-81d290a46ddb",
   "metadata": {},
   "source": [
    "### Anwers\n",
    "\n",
    "1. The .txt file is located in the HDFS, while the script is found in the storage of the local server (jupyter.polito.it)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23400b0-5e77-4ee3-b562-a51c204b2634",
   "metadata": {},
   "source": [
    "## Exercise 2 - Manipulating HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80623b20-b76d-4c66-9da8-3b6eb631cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rwxrwx---+  3 trevisan students         62 2019-09-06 10:15 /data/students/bigdata_internet/lab1/lab1_dataset.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /data/students/bigdata_internet/lab1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbff3b16-fc87-46f0-a573-ef018f29d9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get: `/home/students/s315054/labs/lab01/lab1_dataset.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -get /data/students/bigdata_internet/lab1/lab1_dataset.txt /home/students/s315054/labs/lab01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ad058df-546a-496f-8c49-ade412d5b56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/s315054/emptyFile': File exists\n"
     ]
    }
   ],
   "source": [
    "!touch emptyFile; hdfs dfs -put emptyFile /user/s315054"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f16c61-9fe6-40b4-908c-a81a743340b7",
   "metadata": {},
   "source": [
    "### Answers \n",
    "1. No, since the file was copied and it is not a mirror for the actual one stored in the HDFS\n",
    "2. Path in HDFS: `hdfs://BigDataHA/user/s315054/`; Path on gateway local file system: `/home/students/s315054/`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e20c9c-b593-48f4-9a35-284820eb92ce",
   "metadata": {},
   "source": [
    "## Exercise 3 - Running a job\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cafec9cf-6f1f-4081-8fb2-66a4c84f27c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 15:52:08 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s315054/lab01/results_3' to trash at: hdfs://BigDataHA/user/s315054/.Trash/Current/user/s315054/lab01/results_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bob,11\n",
      "john,21\n"
     ]
    }
   ],
   "source": [
    "# Before running again clear file location (saveAsTextFile does not overwrite)\n",
    "!hdfs dfs -rm -r /user/s315054/lab01/results_3\n",
    "\n",
    "rdd = sc.textFile(\"/data/students/bigdata_internet/lab1/lab1_dataset.txt\")\n",
    "fields_rdd = rdd.map(lambda line: line.split(','))   # Isolate each element of the row\n",
    "reduced_rdd = fields_rdd.reduceByKey(lambda v1, v2: int(v1)+int(v2))   # Evaluate the sum of all values related to the same key\n",
    "# Notice the need of cast\n",
    "correct_rdd = reduced_rdd.map(lambda l: l[0]+','+str(l[1]))\n",
    "sample_loc = correct_rdd.take(2)  # Take 2 elements from the RDD as a way to check the functioning\n",
    "for i in range(len(sample_loc)):\n",
    "    print(sample_loc[i])\n",
    "\n",
    "\n",
    "correct_rdd.saveAsTextFile('/user/s315054/lab01/results_3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab76b8-6700-4032-95b9-7b1e41cb55a4",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "1. The code performs the following:\n",
    "    * Creation of a RDD after reading the file `lab1_dataset.txt`\n",
    "    * Creation of another RDD by isolating, for each row of the initial file, the name and the associated number\n",
    "    * `reduceByKey` operation to sum values associated with the same name (having casted the values to integer)\n",
    "    * Creation of a new RDD in which each key-value pair was converted to a single string\n",
    "    * Creation of a file stored in the HDFS containing each of the elements of the file as lines\n",
    "    * The method `take(2)` was also employed as a way to print a couple of the strings produced (in order to test the correct functioning)\n",
    "\n",
    "2. The output folder contains 2 `.txt` files, one for each partition used to store the final RDD, plus a binary file named `_SUCCESS`, probably containing a log for the `saveAsTextFile()` method\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c11b433-f794-450f-8924-285245fca99e",
   "metadata": {},
   "source": [
    "## Exercise 4 (Bonus task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d604517-0667-49f1-943c-722f912e8bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 16:01:09 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s315054/lab01/results_4' to trash at: hdfs://BigDataHA/user/s315054/.Trash/Current/user/s315054/lab01/results_4\n",
      "bob,5-3-3\n",
      "john,4-8-9\n",
      "alice,4-3-7\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/s315054/lab01/results_4\n",
    "\n",
    "rdd = sc.textFile(\"/data/students/bigdata_internet/lab1/lab1_dataset.txt\")   # Create initial RDD from HDFS file (1 element per row)\n",
    "fields_rdd = rdd.map(lambda line: line.split(','))   # Isolate each element of the row\n",
    "reduced_rdd = fields_rdd.reduceByKey(lambda v1, v2: v1 + '-' + v2)   # Append all values\n",
    "correct_rdd = reduced_rdd.map(lambda l: l[0]+','+str(l[1]))   # Create lines\n",
    "sample_loc = correct_rdd.take(3)  # Take 2 elements from the RDD as a way to check the functioning\n",
    "for i in range(len(sample_loc)):\n",
    "    print(sample_loc[i])\n",
    "\n",
    "\n",
    "correct_rdd.saveAsTextFile('/user/s315054/lab01/results_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a8fd2-2c78-4b85-bc38-8607a9a8fb7a",
   "metadata": {},
   "source": [
    "### Comments\n",
    "The program works pretty much in the same way as the one found in exercise 3, except that instead of using the method `reduceByKey()` to evaluate the sum of all values associated with the same key, it is used to assemble all values into a single string, by appending (`+` operator in python) `'-'` and the following value. Unlike before, in this case it is not necessary to cast the value to an int, since we need to operate with strings in order to perform the append operation.\n",
    "\n",
    "Another possibility could have been that of using `foldByKey`, but since the order of the values was not specified it was more efficient to use `reduceByKey`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
