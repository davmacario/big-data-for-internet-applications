{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b9aa3b-4c69-47c5-b554-ca22048e109b",
   "metadata": {},
   "source": [
    "# Lab 02 - Spark RDD\n",
    "Processing large text files using Spark\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5594c3be-3348-4252-adc0-2a71d52bfad9",
   "metadata": {},
   "source": [
    "# Ex 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e640fefc-8c81-4033-986f-e5afb8ca9072",
   "metadata": {},
   "source": [
    "## Part 1.0 - creating RDD\n",
    "Given the file containing words and respective frequencies, create the input RDD and separate the elements to obtain a pair RDD.\n",
    "(Knowing that the general structure of a .tsv file is: `word\\tword\\n`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da889449-5dc0-4cb6-8c83-b339d894b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/students/bigdata_internet/lab2/word_frequency.tsv'\n",
    "\n",
    "inRDD = sc.textFile(path)\n",
    "sepRDD = inRDD.map(lambda l: (l.split('\\t')[0], int(l.split('\\t')[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81567a0-88a5-4295-a723-721affdf94ff",
   "metadata": {},
   "source": [
    "### 1.0.1 - Draw 5 (random) samples from the RDD.\n",
    "The `takeSample()` method is called with `False` as a parameter to take the elements without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3362b8-ec34-4fd6-b18d-e0f79efb7173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Roast;', 30), ('soft-its', 1), ('Olives\"', 2), ('most\"', 2), ('oil-suspension', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Drawing 5 random samples\n",
    "out_five = sepRDD.takeSample(False, 5)\n",
    "print(out_five)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e01ed09-d7d8-4b69-b16e-5fcc0204839a",
   "metadata": {},
   "source": [
    "### 1.0.2 - Pick the first 5 words in order of frequency (use top).\n",
    "`top()` allows a function to be passed to choose the ordering to be used: in this case, since we want the most frequent words (words with highest value), the function only needs to extract the value from each pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48e88c3-4da6-4257-ac26-baecea130868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1630750), ('I', 1448619), ('and', 1237250), ('a', 1164419), ('to', 997979)]\n"
     ]
    }
   ],
   "source": [
    "# Drawing the 5 most recurring words\n",
    "out_top = sepRDD.top(5, lambda couple: couple[1])\n",
    "print(out_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a20966-dc12-48b4-a429-579dfffb4809",
   "metadata": {},
   "source": [
    "### 1.0.3 - Count how many elements the file contains.\n",
    "It is possible to use the `count()` method on the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74f0011a-bf77-4fca-91f7-62f13316ef1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 339819 elements inside the list\n"
     ]
    }
   ],
   "source": [
    "# Count words\n",
    "n_elem = sepRDD.count()\n",
    "print(f\"There are {n_elem} elements inside the list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc40f84-c6e7-4763-a2e8-b7c76bf3cb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45444841\n"
     ]
    }
   ],
   "source": [
    "# Total number of words (for comparison in part 3.2)\n",
    "numbersRDD = sepRDD.map(lambda couple: int(couple[1]))\n",
    "totalwords_1 = numbersRDD.reduce(lambda couple1, couple2: couple1 + couple2)\n",
    "print(totalwords_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7aea0c-cc79-4248-aba2-74feee2fa6e1",
   "metadata": {},
   "source": [
    "### 1.0.4 - Observe `word_frequency.tsv`.\n",
    "It is actually a folder, which contains `\\_SUCCESS` (a Spark log file), `part-00000` and `part-00001`, which are the typical files found in the output of a `saveAsTextFile()` operation. We can infer that the RDD which originated this file was stored into 2 partitions distributed among the working nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68954a2-2327-456e-85ee-7a0eacfc4912",
   "metadata": {},
   "source": [
    "## Part 1.1 - Filtering words starting with a specified prefix\n",
    "\n",
    "Define the prefix (`'ho'`) and filter RDD to only keep elements whose key starts with the specified string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c586b17-dd99-498d-94ab-dbf1c647eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'ho'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c16b6a3-91e5-48d9-bc38-6afe9c819934",
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredRDD = sepRDD.filter(lambda couple: couple[0].startswith(prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33d389-74e1-4dc0-8c28-c55d0be0a0e3",
   "metadata": {},
   "source": [
    "### 1.1.1\n",
    "Having filtered the RDD, count how many elements are left - using the `count()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fa41f1d-8dc3-4dda-a178-70ad63ed11f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filtered RDD contains 1519 words\n"
     ]
    }
   ],
   "source": [
    "n_filtered = filteredRDD.count()\n",
    "print(f\"The filtered RDD contains {n_filtered} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a41510a-50bb-46bb-b6d5-0ff7a6741de7",
   "metadata": {},
   "source": [
    "### 1.1.2\n",
    "Find out how frequent is the most frequent word of the filtered RDD.\n",
    "(this is one of the possible ways - see part 1.1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8941072-73de-4b15-923b-36bbeb609e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent word in the filtered RDD (words beginning with 'ho') has frequency 36264\n"
     ]
    }
   ],
   "source": [
    "most_freq_1 = filteredRDD.max(lambda couple: couple[1])\n",
    "print(f\"The most frequent word in the filtered RDD (words beginning with '{prefix}') has frequency {most_freq_1[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632f864-a010-4f14-a675-fcd135e0109c",
   "metadata": {},
   "source": [
    "### 1.1.3\n",
    "Other 2 ways to evaluate the same value as in point 1.1.2\n",
    "* `top()` method\n",
    "* `first()` method, after sorting the RDD with the `sortBy()` transformation. Sorting is performed according to `-1*value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7243f6a0-0467-498e-98e2-7df654651084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36264\n"
     ]
    }
   ],
   "source": [
    "most_freq_2 = filteredRDD.top(1, lambda couple: couple[1])[0]\n",
    "print(most_freq_2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebcc4ed8-5317-440b-90ca-c41c3607d72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36264\n"
     ]
    }
   ],
   "source": [
    "sortedRDD = filteredRDD.sortBy(lambda couple: -1*couple[1])\n",
    "most_freq_3 = sortedRDD.first()\n",
    "print(most_freq_3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38078e1f-e184-470b-ae23-978a37c7d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one of the three values just found\n",
    "maxfreq = most_freq_1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c968b-f950-4609-a393-f795d3a5f8f4",
   "metadata": {},
   "source": [
    "## Part 1.2 - Filter most frequent words\n",
    "Set the frequency threshold (`freq`) to 70% of the highest frequency (`maxfreq`) found in the point before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ec388bb-539c-4b59-87c7-c6843bb027cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The threshold value has been set to 25384.8\n"
     ]
    }
   ],
   "source": [
    "freq = .7*maxfreq\n",
    "print(f\"The threshold value has been set to {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ec944e0-12a9-4f5d-9085-3b7c0ae82760",
   "metadata": {},
   "outputs": [],
   "source": [
    "topFreqRDD = filteredRDD.filter(lambda line: line[1] >= freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c2c019-014b-48ff-b37e-7338b3ade389",
   "metadata": {},
   "source": [
    "## Part 1.3 - Count the remaining words and save the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5adbae-f89c-4185-bfba-424e7be77cb5",
   "metadata": {},
   "source": [
    "### 1.3.1\n",
    "Count how many elements are left after both filtering operations (`count()` method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dae0843b-48fd-46b3-88c1-ea103958faeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The remaining elements are 2\n"
     ]
    }
   ],
   "source": [
    "n_remaining_words = topFreqRDD.count()\n",
    "print(f\"The remaining elements are {n_remaining_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31835548-1bf1-48dc-9683-f96bffdec456",
   "metadata": {},
   "source": [
    "### 1.3.2\n",
    "Isolate keys and store the words on an output file (`/user/s315054/lab02/results_01`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b99221b-788f-4a4d-a52f-2fe1bbff6bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 16:33:38 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s315054/lab02/results_01.txt' to trash at: hdfs://BigDataHA/user/s315054/.Trash/Current/user/s315054/lab02/results_01.txt\n"
     ]
    }
   ],
   "source": [
    "# This line is just used to clear the content of the output folder to prevent overwriting\n",
    "!hdfs dfs -rm -r /user/s315054/lab02/results_01.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d3c1602-8884-4fbc-9a53-3039437daf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hot;', 'how;']\n"
     ]
    }
   ],
   "source": [
    "outRDD = topFreqRDD.map(lambda c: c[0]+';')\n",
    "print(outRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f77faf02-e972-458d-b214-bde3a625e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outPath = '/user/s315054/lab02/results_01.txt'\n",
    "outRDD.saveAsTextFile(outPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9789b9a8-af36-478e-8b56-3826a97d1fea",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b044b93c-7d4f-46ef-b469-62da0356abb2",
   "metadata": {},
   "source": [
    "# Ex 2\n",
    "\n",
    "The program `lab02_ex02.py` is ran from terminal using `spark-submit`.\n",
    "Below is reported the program code:\n",
    "\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    import sys\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    # The prefix is passed as a command line argument (first one)\n",
    "    prefix = sys.argv[1]\n",
    "    # The output path is the second command line argument \n",
    "    outputPath = sys.argv[2]\n",
    "    # Input file path (HDFS)\n",
    "    path = '/data/students/bigdata_internet/lab2/word_frequency.tsv'\n",
    "    # Create SparkContext object\n",
    "    conf = SparkConf().setAppName('Exercise 02, lab 02')\n",
    "    sc = SparkContext(conf=conf)\n",
    "    # Create pair RDD (sepRDD)\n",
    "    inRDD = sc.textFile(path)\n",
    "    sepRDD = inRDD.map(lambda l: (l.split('\\t')[0], int(l.split('\\t')[1])))\n",
    "    # Isolate elements whose key starts with the specified prefix\n",
    "    filteredRDD = sepRDD.filter(lambda couple: couple[0].startswith(prefix))\n",
    "    # Produce output file\n",
    "    outRDD = filteredRDD.map(lambda c: c[0]+' - '+str(c[1])+',')\n",
    "    outRDD.saveAsTextFile(outputPath)\n",
    "    stop = time.time()\n",
    "    print(f\"The program takes {stop-start} seconds to run\")\n",
    "\n",
    "\n",
    "The following cells contain terminal commands (using character `!`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aede4166-be5c-4f95-9efc-fa8723467332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/11 16:36:14 INFO fs.TrashPolicyDefault: Moved: 'hdfs://BigDataHA/user/s315054/lab02/results_02.txt' to trash at: hdfs://BigDataHA/user/s315054/.Trash/Current/user/s315054/lab02/results_02.txt1673454974466\n"
     ]
    }
   ],
   "source": [
    "# This line is just used to clear the content of the output folder to prevent overwriting\n",
    "!hdfs dfs -rm -r /user/s315054/lab02/results_02.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf924480-075d-4cac-90ad-be01712887ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1425774/lib/spark) overrides detected (/opt/cloudera/parcels/CDH/lib/spark).\n",
      "WARNING: Running spark-class from user-defined location.\n",
      "23/01/11 16:35:16 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/01/11 16:35:16 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/01/11 16:35:16 WARN util.Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "The program takes 4.231187343597412 seconds to run                              \n"
     ]
    }
   ],
   "source": [
    "# Running locally (on jupyter.polito.it)\n",
    "!spark-submit --master local --deploy-mode client lab02_ex02.py ho /user/s315054/lab02/results_02.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bbcfdd1-f054-4e91-afda-bdc7473bbb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/CDH-6.2.1-1.cdh6.2.1.p0.1425774/lib/spark) overrides detected (/opt/cloudera/parcels/CDH/lib/spark).\n",
      "WARNING: Running spark-class from user-defined location.\n",
      "23/01/11 16:36:18 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/01/11 16:36:18 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/01/11 16:36:18 WARN util.Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/01/11 16:36:27 WARN cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "The program takes 18.167657136917114 seconds to run                             \n"
     ]
    }
   ],
   "source": [
    "# Running on the cluster\n",
    "!spark-submit --master yarn lab02_ex02.py ho /user/s315054/lab02/results_02.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30d673-1431-436f-a0fa-d6c1572188ca",
   "metadata": {},
   "source": [
    "## 2.1\n",
    "When ran locally, the time taken is approximately 4.2 seconds, while, when ran in the cluster (`--master yarn`), the time is about 18.2 seconds, probably due to the YARN scheduler taking time in parallelizing the tasks. Indeed, the dataset is not excessively large and running the tasks locally is still more convenient than than running them on the cluster nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a8efc-88c0-4c3e-a2d2-6c731921b794",
   "metadata": {},
   "source": [
    "## 2.2\n",
    "\n",
    "In this case, since we are executing only a single action (`saveAsTextFile()`), caching would not improve performance, as all transformations are executed just once, as we reach the line in which this action is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c8d38-820f-40e7-b08a-a81ce4da860d",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7539a312-e1db-46ed-8b8b-2ef2d9c79304",
   "metadata": {},
   "source": [
    "# Ex 3 - Bonus Task\n",
    "\n",
    "Analyze the full file from which the pairs (word, frequency) were obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ced67ff5-d223-403c-ac2b-723cc756d590",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/data/students/bigdata_internet/lab2/finefoods_text.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8da5fe8-39ae-4ce4-950c-d111812408af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file\n",
    "inFullRDD = sc.textFile(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bddf5319-f09e-4a9b-ae25-0ba0d6b783a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove character '\\x0c'\n",
    "procRDD = inFullRDD.map(lambda line: line.replace('\\x0c', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac314c07-ee25-42bb-8cbf-d416c5a8399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words is 45444841 (including duplicates)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Separate the words\n",
    "allwordsRDD = procRDD.flatMap(lambda line: line.strip().split(' '))\n",
    "\n",
    "## Issue: when multiple subsequent blankspaces are present, \n",
    "# empty strings are added to the set (they are more than 5 million)\n",
    "# Remove empty strings:\n",
    "actualWordsRDD = allwordsRDD.filter(lambda word: word != '')\n",
    "\n",
    "# Count the elements\n",
    "totalWords = actualWordsRDD.count()\n",
    "print(f\"The total number of words is {totalWords} (including duplicates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24766b9f-210f-40b8-a86a-8c29cfb79eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows of the input file is 568454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nrows = inFullRDD.count()\n",
    "print(f\"The number of rows of the input file is {nrows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8b15eb-37e7-4a7d-b694-e61c19404e63",
   "metadata": {},
   "source": [
    "## 3.1\n",
    "\n",
    "As highlighted, the total number of words present is 45444841, which corresponds to the number of words obtained by summing all recurrences from the file of frequencies.\n",
    "\n",
    "It is to be noted, however, that this file needed particular attention, since probably the removal of punctuation marks was simply carried out by replacing them with blankspaces, meaning that often multiple consecutive blankspaces appear. This, when splitting the lines as `line.split(' ')`, causes the program to wrongly include as words empty strings (`''`). For this reason it ws necessary to filter them out.\n",
    "\n",
    "Another step was that of removing the 'form feed' character `\\x0c`, which apparently was not removed before. The reasons for this will be clear in point 3.2, but basically this special sequence was causing the program to misinterpret one word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e46f70-44a8-4f0b-850e-8fe99660ecf6",
   "metadata": {},
   "source": [
    "## 3.2\n",
    "The following cells contain the steps used to obtain the frequencies file from the words isolated. Then, by means of a `subtract()` method call it was possible to verify that the two files were in fact identical. \n",
    "\n",
    "(Notice that `sepRDD` was created at the beginning of this lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f66d7fe-c8f9-4820-b662-35aaa0f114a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpRDD = actualWordsRDD.map(lambda word: (word, 1))\n",
    "frequencyRDD = tmpRDD.reduceByKey(lambda v1, v2: v1+v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fc73922-4047-4d04-b1cc-44a1edffefdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of different elements is: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check that the elements are actually the same\n",
    "diffRDD = frequencyRDD.subtract(sepRDD)\n",
    "check = diffRDD.count()\n",
    "print(f\"The number of different elements is: {check}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6bc0c2-b12f-4a26-9e08-3616e1a1ddc5",
   "metadata": {},
   "source": [
    "During the creation of the program, it was found that one element was different between the frequencies file and the version obtained from the full review file. Upon further inspection, it turned out that one word was containing the special character (form feed) `\\x0c`, which prevented it to be reunited with the same words when applying the `reduceByKey()` operation.\n",
    "Then, as shown in point 3.1, the character was removed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
